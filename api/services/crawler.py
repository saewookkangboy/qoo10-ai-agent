"""
Qoo10 í¬ë¡¤ë§ ì„œë¹„ìŠ¤ (AI ê°•í™” í•™ìŠµ ë° ë°©í™”ë²½ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨)
Qoo10 ìƒí’ˆ ë° Shop í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë©°, í•™ìŠµì„ í†µí•´ ì„±ëŠ¥ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤.

í¬ë¡¤ë§ ì›ì¹™:
- CRAWLING_ANALYSIS_PRINCIPLES.md ì°¸ì¡°
- Playwright í¬ë¡¤ë§ì„ ê¸°ë³¸ ê¶Œì¥ (ë™ì  ì½˜í…ì¸  ì¶”ì¶œ)
- ëª¨ë“  í¬ë¡¤ë§ ê²°ê³¼ì— crawled_with í•„ë“œ í¬í•¨
- ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™” ê·œì¹™ ì¤€ìˆ˜
"""
import httpx
from bs4 import BeautifulSoup
import re
from typing import Dict, List, Optional, Any
import asyncio
from urllib.parse import urlparse
import random
import time
from datetime import datetime
import os
import logging
from pathlib import Path
from dotenv import load_dotenv
import json

from services.database import CrawlerDatabase
from .crawler_shop import ShopCrawlerMixin

load_dotenv()

_logger = logging.getLogger(__name__)

from services.logging_utils import log_debug as _log_debug

# Qoo10 API ì„œë¹„ìŠ¤ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from .qoo10_api_service import Qoo10APIService
    QOO10_API_AVAILABLE = True
except ImportError:
    QOO10_API_AVAILABLE = False
    Qoo10APIService = None

# Qoo10 API ìŠ¤í‚¤ë§ˆ ì„í¬íŠ¸ (API êµ¬ì¡° ì°¸ê³ )
try:
    from .qoo10_api_schema import Qoo10APISchema
    API_SCHEMA_AVAILABLE = True
except ImportError:
    API_SCHEMA_AVAILABLE = False
    Qoo10APISchema = None

# Playwright ì„í¬íŠ¸ (ì„ íƒì )
try:
    from playwright.async_api import async_playwright, Browser, Page, TimeoutError as PlaywrightTimeoutError
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False


# httpx ë²„ì „ë³„ proxies ì§€ì› ì—¬ë¶€ (ëª¨ë“ˆ ë¡œë“œ ì‹œ 1íšŒ íŒë³„)
try:
    import inspect as _inspect

    _HTTPX_SUPPORTS_PROXIES_PARAM = "proxies" in _inspect.signature(httpx.AsyncClient.__init__).parameters
except Exception:
    _HTTPX_SUPPORTS_PROXIES_PARAM = True  # ë³´ìˆ˜ì ìœ¼ë¡œ ê¸°ë³¸ê°’ True


class Qoo10Crawler(ShopCrawlerMixin):
    """Qoo10 í˜ì´ì§€ í¬ë¡¤ëŸ¬ (AI ê°•í™” í•™ìŠµ ë° ë°©í™”ë²½ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨)

    ì£¼ì˜: ì¸ìŠ¤í„´ìŠ¤ëŠ” **ë‹¨ì¼ ì‘ì—…/ë‹¨ì¼ ì½”ë£¨í‹´**ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì „ì œë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
    ë™ì¼í•œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì—¬ëŸ¬ ì½”ë£¨í‹´ì—ì„œ ë™ì‹œì— ì‚¬ìš©í•˜ë©´ User-Agent, í”„ë¡ì‹œ, ì„¸ì…˜ ì¿ í‚¤
    ìƒíƒœê°€ ê³µìœ ë˜ì–´ ì˜ˆê¸°ì¹˜ ì•Šì€ ë™ì‘ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì‹œì— ì—¬ëŸ¬ URLì„
    í¬ë¡¤ë§í•´ì•¼ í•œë‹¤ë©´ `Qoo10Crawler` ì¸ìŠ¤í„´ìŠ¤ë¥¼ URL/ì‘ì—…ë³„ë¡œ ë¶„ë¦¬í•´ì„œ ìƒì„±í•˜ì„¸ìš”.
    """
    
    # ì¼ë³¸ì–´-í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    JP_KR_MAPPING = {
        # ê°€ê²© ê´€ë ¨
        "å•†å“ä¾¡æ ¼": "ìƒí’ˆê°€ê²©",
        "ä¾¡æ ¼": "ê°€ê²©",
        "å®šä¾¡": "ì •ê°€",
        "å…ƒã®ä¾¡æ ¼": "ì›ë˜ê°€ê²©",
        "å…ƒä¾¡æ ¼": "ì›ê°€ê²©",
        "è²©å£²ä¾¡æ ¼": "íŒë§¤ê°€ê²©",
        "ã‚»ãƒ¼ãƒ«ä¾¡æ ¼": "ì„¸ì¼ê°€ê²©",
        "å‰²å¼•ä¾¡æ ¼": "í• ì¸ê°€ê²©",
        "å††": "ì—”",
        
        # ë°°ì†¡ ê´€ë ¨
        "é€æ–™": "ë°°ì†¡ë¹„",
        "é€æ–™ç„¡æ–™": "ë¬´ë£Œë°°ì†¡",
        "é…é€æ–™": "ë°°ì†¡ë£Œ",
        "é…é€": "ë°°ì†¡",
        "é…é€ç„¡æ–™": "ë¬´ë£Œë°°ì†¡",
        "æ¡ä»¶ä»˜ç„¡æ–™": "ì¡°ê±´ë¶€ë¬´ë£Œ",
        "ä»¥ä¸Šè³¼å…¥": "ì´ìƒêµ¬ë§¤",
        "ä»¥ä¸Šè³¼å…¥ã®éš›": "ì´ìƒêµ¬ë§¤ì‹œ",
        "è³¼å…¥": "êµ¬ë§¤",
        
        # ë¦¬ë·° ê´€ë ¨
        "ãƒ¬ãƒ“ãƒ¥ãƒ¼": "ë¦¬ë·°",
        "è©•ä¾¡": "í‰ê°€",
        "ã‚³ãƒ¡ãƒ³ãƒˆ": "ì½”ë©˜íŠ¸",
        "å£ã‚³ãƒŸ": "êµ¬ì „",
        "æ˜Ÿ": "ë³„",
        "è©•ä¾¡æ•°": "í‰ê°€ìˆ˜",
        
        # ì¿ í°/í• ì¸ ê´€ë ¨
        "ã‚¯ãƒ¼ãƒãƒ³": "ì¿ í°",
        "å‰²å¼•": "í• ì¸",
        "ã‚¯ãƒ¼ãƒãƒ³å‰²å¼•": "ì¿ í°í• ì¸",
        "ã‚·ãƒ§ãƒƒãƒ—ãŠæ°—ã«å…¥ã‚Šå‰²å¼•": "ìƒµì¦ê²¨ì°¾ê¸°í• ì¸",
        "ãŠæ°—ã«å…¥ã‚Šç™»éŒ²": "ì¦ê²¨ì°¾ê¸°ë“±ë¡",
        "ãƒ—ãƒ©ã‚¹": "í”ŒëŸ¬ìŠ¤",
        "æœ€å¤§": "ìµœëŒ€",
        "off": "ì˜¤í”„",
        "OFF": "ì˜¤í”„",
        
        # Qãƒã‚¤ãƒ³ãƒˆ ê´€ë ¨
        "Qãƒã‚¤ãƒ³ãƒˆ": "Qí¬ì¸íŠ¸",
        "ãƒã‚¤ãƒ³ãƒˆ": "í¬ì¸íŠ¸",
        "Qãƒã‚¤ãƒ³ãƒˆç²å¾—": "Qí¬ì¸íŠ¸íšë“",
        "Qãƒã‚¤ãƒ³ãƒˆç²å¾—æ–¹æ³•": "Qí¬ì¸íŠ¸íšë“ë°©ë²•",
        "å—å–ç¢ºèª": "ìˆ˜ë ¹í™•ì¸",
        "ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ": "ë¦¬ë·°ì‘ì„±",
        "é…é€å®Œäº†": "ë°°ì†¡ì™„ë£Œ",
        "è‡ªå‹•": "ìë™",
        
        # ë°˜í’ˆ ê´€ë ¨
        "è¿”å“": "ë°˜í’ˆ",
        "è¿”å“ç„¡æ–™": "ë¬´ë£Œë°˜í’ˆ",
        "ç„¡æ–™è¿”å“": "ë¬´ë£Œë°˜í’ˆ",
        "è¿”å“ç„¡æ–™ã‚µãƒ¼ãƒ“ã‚¹": "ë¬´ë£Œë°˜í’ˆì„œë¹„ìŠ¤",
        "è¿”å´": "ë°˜í™˜",
        "è¿”å“å¯èƒ½": "ë°˜í’ˆê°€ëŠ¥",
        
        # ìƒí’ˆ ê´€ë ¨
        "å•†å“": "ìƒí’ˆ",
        "å•†å“å": "ìƒí’ˆëª…",
        "å•†å“èª¬æ˜": "ìƒí’ˆì„¤ëª…",
        "å•†å“è©³ç´°": "ìƒí’ˆìƒì„¸",
        "å•†å“æƒ…å ±": "ìƒí’ˆì •ë³´",
        "å•†å“ç”»åƒ": "ìƒí’ˆì´ë¯¸ì§€",
        "å•†å“ç•ªå·": "ìƒí’ˆë²ˆí˜¸",
        "å•†å“ã‚³ãƒ¼ãƒ‰": "ìƒí’ˆì½”ë“œ",
        
        # Shop ê´€ë ¨
        "ã‚·ãƒ§ãƒƒãƒ—": "ìƒµ",
        "ã‚·ãƒ§ãƒƒãƒ—å": "ìƒµëª…",
        "ã‚·ãƒ§ãƒƒãƒ—æƒ…å ±": "ìƒµì •ë³´",
        "ã‚·ãƒ§ãƒƒãƒ—ãƒšãƒ¼ã‚¸": "ìƒµí˜ì´ì§€",
        "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼": "íŒ”ë¡œì›Œ",
        "ãƒ•ã‚©ãƒ­ãƒ¼": "íŒ”ë¡œìš°",
        "ãƒ•ã‚©ãƒ­ãƒ¼ä¸­": "íŒ”ë¡œìš°ì¤‘",
        
        # ì¹´í…Œê³ ë¦¬/ë¸Œëœë“œ ê´€ë ¨
        "ã‚«ãƒ†ã‚´ãƒª": "ì¹´í…Œê³ ë¦¬",
        "ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ì¹´í…Œê³ ë¦¬",
        "ãƒ–ãƒ©ãƒ³ãƒ‰": "ë¸Œëœë“œ",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼": "ë©”ì´ì»¤",
        
        # ê¸°íƒ€
        "å…¨ã¦ã®å•†å“": "ì „ì²´ìƒí’ˆ",
        "å…¨ã¦": "ì „ì²´",
        "æ¤œç´¢": "ê²€ìƒ‰",
        "æ¤œç´¢çµæœ": "ê²€ìƒ‰ê²°ê³¼",
        "äººæ°—": "ì¸ê¸°",
        "æ–°ç€": "ì‹ ê·œ",
        "ãƒ©ãƒ³ã‚­ãƒ³ã‚°": "ë­í‚¹",
        "ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«": "íƒ€ì„ì„¸ì¼",
        "ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ«ä¸­": "íƒ€ì„ì„¸ì¼ì¤‘",
        "MOVE": "ë¬´ë¸Œ",
        "POWER": "íŒŒì›Œ",
        "ãƒ‘ãƒ¯ãƒ¼": "íŒŒì›Œ",
        "ã‚°ãƒ¬ãƒ¼ãƒ‰": "ê·¸ë ˆì´ë“œ",
        "byPower": "ë°”ì´íŒŒì›Œ",
        "by Power": "ë°”ì´íŒŒì›Œ",
    }
    
    # ë‹¤ì–‘í•œ User-Agent ëª©ë¡
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]
    
    def __init__(self, db: Optional[CrawlerDatabase] = None, error_reporting_service=None):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            error_reporting_service: ì˜¤ë¥˜ ì‹ ê³  ì„œë¹„ìŠ¤ (ìš°ì„  í¬ë¡¤ë§ìš©)
        """
        self.base_url = "https://www.qoo10.jp"
        self.timeout = 15.0  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•: 30ì´ˆ -> 15ì´ˆ
        self.max_retries = 2  # ì¬ì‹œë„ íšŸìˆ˜ ê°ì†Œ: 3 -> 2
        self.retry_delay_base = 1.0  # ì¬ì‹œë„ ì§€ì—° ì‹œê°„ ë‹¨ì¶•: 2ì´ˆ -> 1ì´ˆ
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db = db or CrawlerDatabase()
        
        # ì˜¤ë¥˜ ì‹ ê³  ì„œë¹„ìŠ¤ (ìš°ì„  í¬ë¡¤ë§ìš©)
        self.error_reporting_service = error_reporting_service
        
        # í”„ë¡ì‹œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°)
        self.proxies = self._load_proxies()
        
        # ì¼ë³¸ì–´-í•œêµ­ì–´ íŒ¨í„´ ìƒì„±
        self._init_jp_kr_patterns()
        
        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ User-Agent ë° í”„ë¡ì‹œ
        self.current_user_agent = None
        self.current_proxy = None
        
        # ì„¸ì…˜ ê´€ë¦¬
        self.session_cookies = {}
        
        # Playwright ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ (í•„ìš”ì‹œ ì´ˆê¸°í™”)
        self._playwright_browser = None
        self._playwright_context = None
        
        # ìš°ì„  í¬ë¡¤ë§ í•„ë“œ ëª©ë¡ (ì˜¤ë¥˜ ì‹ ê³ ëœ í•„ë“œ)
        self._priority_fields = None
        self._priority_chunks = {}  # í•„ë“œë³„ Chunk ì •ë³´
        
        # Qoo10 API ì„œë¹„ìŠ¤ (ì„ íƒì )
        self.api_service = None
        if QOO10_API_AVAILABLE:
            try:
                self.api_service = Qoo10APIService()
                if self.api_service.certification_key:
                    _logger.info("Qoo10 API ì„œë¹„ìŠ¤ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                _logger.warning(f"Qoo10 API ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e!s}")
    def _load_proxies(self) -> List[str]:
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡ì‹œ ëª©ë¡ ë¡œë“œ"""
        proxy_list = os.getenv("PROXY_LIST", "")
        if proxy_list:
            return [p.strip() for p in proxy_list.split(",") if p.strip()]
        return []
    
    def _init_jp_kr_patterns(self):
        """ì¼ë³¸ì–´-í•œêµ­ì–´ íŒ¨í„´ ì´ˆê¸°í™”"""
        # ì¼ë³¸ì–´ì™€ í•œêµ­ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì •ê·œì‹ íŒ¨í„´ ìƒì„±
        self.jp_kr_patterns = {}
        
        # ê°€ê²© ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["price"] = r'(å•†å“ä¾¡æ ¼|ìƒí’ˆê°€ê²©|ê°€ê²©|ä¾¡æ ¼)[ï¼š:]?\s*'
        self.jp_kr_patterns["original_price"] = r'(å®šä¾¡|ì •ê°€|å…ƒã®ä¾¡æ ¼|ì›ë˜ê°€ê²©|å…ƒä¾¡æ ¼|ì›ê°€ê²©)[ï¼š:]?\s*'
        self.jp_kr_patterns["sale_price"] = r'(è²©å£²ä¾¡æ ¼|íŒë§¤ê°€ê²©|ã‚»ãƒ¼ãƒ«ä¾¡æ ¼|ì„¸ì¼ê°€ê²©|å‰²å¼•ä¾¡æ ¼|í• ì¸ê°€ê²©)[ï¼š:]?\s*'
        
        # ë°°ì†¡ ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["shipping"] = r'(é€æ–™|ë°°ì†¡ë¹„|é…é€æ–™|ë°°ì†¡ë£Œ|é…é€|ë°°ì†¡)[ï¼š:]?\s*'
        self.jp_kr_patterns["free_shipping"] = r'(é€æ–™ç„¡æ–™|ë¬´ë£Œë°°ì†¡|é…é€ç„¡æ–™|ç„¡æ–™é…é€|æ¡ä»¶ä»˜ç„¡æ–™|ì¡°ê±´ë¶€ë¬´ë£Œ)'
        self.jp_kr_patterns["shipping_threshold"] = r'(\d{1,3}(?:,\d{3})*)å††\s*(ä»¥ä¸Šè³¼å…¥|ì´ìƒêµ¬ë§¤|ä»¥ä¸Šè³¼å…¥ã®éš›|ì´ìƒêµ¬ë§¤ì‹œ)'
        
        # ë¦¬ë·° ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["review"] = r'(ãƒ¬ãƒ“ãƒ¥ãƒ¼|ë¦¬ë·°|è©•ä¾¡|í‰ê°€|ã‚³ãƒ¡ãƒ³ãƒˆ|ì½”ë©˜íŠ¸|å£ã‚³ãƒŸ|êµ¬ì „)'
        self.jp_kr_patterns["review_count"] = r'(ãƒ¬ãƒ“ãƒ¥ãƒ¼|ë¦¬ë·°|è©•ä¾¡|í‰ê°€|è©•ä¾¡æ•°|í‰ê°€ìˆ˜).*?\((\d+)\)'
        
        # ì¿ í° ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["coupon"] = r'(ã‚¯ãƒ¼ãƒãƒ³|ì¿ í°|å‰²å¼•|í• ì¸|ã‚¯ãƒ¼ãƒãƒ³å‰²å¼•|ì¿ í°í• ì¸)'
        self.jp_kr_patterns["coupon_discount"] = r'(ãƒ—ãƒ©ã‚¹|í”ŒëŸ¬ìŠ¤|æœ€å¤§|ìµœëŒ€)\s*(\d+)(å‰²å¼•|í• ì¸|å††|ì—”)'
        self.jp_kr_patterns["shop_favorite_coupon"] = r'(ã‚·ãƒ§ãƒƒãƒ—ãŠæ°—ã«å…¥ã‚Šå‰²å¼•|ìƒµì¦ê²¨ì°¾ê¸°í• ì¸|ãŠæ°—ã«å…¥ã‚Šç™»éŒ²|ì¦ê²¨ì°¾ê¸°ë“±ë¡)'
        
        # Qãƒã‚¤ãƒ³ãƒˆ ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["qpoint"] = r'(Qãƒã‚¤ãƒ³ãƒˆ|Qí¬ì¸íŠ¸|ãƒã‚¤ãƒ³ãƒˆ|í¬ì¸íŠ¸)'
        self.jp_kr_patterns["qpoint_method"] = r'(Qãƒã‚¤ãƒ³ãƒˆç²å¾—æ–¹æ³•|Qí¬ì¸íŠ¸íšë“ë°©ë²•|Qãƒã‚¤ãƒ³ãƒˆç²å¾—|Qí¬ì¸íŠ¸íšë“)'
        self.jp_kr_patterns["qpoint_receive"] = r'(å—å–ç¢ºèª|ìˆ˜ë ¹í™•ì¸)[ï¼š:]?\s*æœ€å¤§(\d+)P'
        self.jp_kr_patterns["qpoint_review"] = r'(ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ|ë¦¬ë·°ì‘ì„±)[ï¼š:]?\s*æœ€å¤§(\d+)P'
        self.jp_kr_patterns["qpoint_auto"] = r'(é…é€å®Œäº†|ë°°ì†¡ì™„ë£Œ).*?(è‡ªå‹•|ìë™).*?(\d+)P'
        
        # ë°˜í’ˆ ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["return"] = r'(è¿”å“|ë°˜í’ˆ|è¿”å´|ë°˜í™˜)'
        self.jp_kr_patterns["free_return"] = r'(è¿”å“ç„¡æ–™|ë¬´ë£Œë°˜í’ˆ|ç„¡æ–™è¿”å“|è¿”å“ç„¡æ–™ã‚µãƒ¼ãƒ“ã‚¹|ë¬´ë£Œë°˜í’ˆì„œë¹„ìŠ¤)'
        
        # Shop ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["shop"] = r'(ã‚·ãƒ§ãƒƒãƒ—|ìƒµ|ã‚·ãƒ§ãƒƒãƒ—å|ìƒµëª…)'
        self.jp_kr_patterns["follower"] = r'(ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|íŒ”ë¡œì›Œ|ãƒ•ã‚©ãƒ­ãƒ¼|íŒ”ë¡œìš°)'
        self.jp_kr_patterns["power"] = r'(POWER|ãƒ‘ãƒ¯ãƒ¼|íŒŒì›Œ)'
        
        # ìƒí’ˆ ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["product"] = r'(å•†å“|ìƒí’ˆ|å•†å“å|ìƒí’ˆëª…)'
        self.jp_kr_patterns["product_count"] = r'(å…¨ã¦ã®å•†å“|ì „ì²´ìƒí’ˆ|å•†å“æ•°|ìƒí’ˆìˆ˜).*?\((\d+)\)'
        
        # ì¹´í…Œê³ ë¦¬/ë¸Œëœë“œ ê´€ë ¨ íŒ¨í„´
        self.jp_kr_patterns["category"] = r'(ã‚«ãƒ†ã‚´ãƒª|ì¹´í…Œê³ ë¦¬|ã‚«ãƒ†ã‚´ãƒªãƒ¼)'
        self.jp_kr_patterns["brand"] = r'(ãƒ–ãƒ©ãƒ³ãƒ‰|ë¸Œëœë“œ|ãƒ¡ãƒ¼ã‚«ãƒ¼|ë©”ì´ì»¤)'
    
    def _translate_jp_to_kr(self, text: str) -> str:
        """ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ë§¤í•‘ ê¸°ë°˜)"""
        if not text:
            return text
        
        translated = text
        for jp, kr in self.JP_KR_MAPPING.items():
            translated = translated.replace(jp, kr)
        
        return translated
    
    def _create_jp_kr_regex(self, jp_text: str, kr_text: str = None) -> str:
        """ì¼ë³¸ì–´ì™€ í•œêµ­ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì •ê·œì‹ íŒ¨í„´ ìƒì„±"""
        if kr_text is None:
            kr_text = self._translate_jp_to_kr(jp_text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        jp_escaped = re.escape(jp_text)
        kr_escaped = re.escape(kr_text)
        
        return f'({jp_escaped}|{kr_escaped})'
    
    def _get_user_agent(self) -> str:
        """ìµœì ì˜ User-Agent ì„ íƒ (í•™ìŠµ ë°ì´í„° ê¸°ë°˜) - ìµœì í™”: ìºì‹±"""
        # ìºì‹œëœ User-Agentê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if self.current_user_agent:
            return self.current_user_agent
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒëŠ” ìµœì†Œí™” (ì„±ëŠ¥ ìµœì í™”)
        try:
            best_ua = self.db.get_best_user_agent()
            if best_ua:
                self.current_user_agent = best_ua
                return best_ua
        except:
            # DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            pass
        
        # ì—†ìœ¼ë©´ ëœë¤ ì„ íƒ
        self.current_user_agent = random.choice(self.USER_AGENTS)
        return self.current_user_agent
    
    def _get_proxy(self) -> Optional[Dict[str, str]]:
        """ìµœì ì˜ í”„ë¡ì‹œ ì„ íƒ (í•™ìŠµ ë°ì´í„° ê¸°ë°˜) - ìµœì í™”: ìºì‹±"""
        if not self.proxies:
            return None
        
        # ìºì‹œëœ í”„ë¡ì‹œê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if self.current_proxy:
            return {"http://": self.current_proxy, "https://": self.current_proxy}
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒëŠ” ìµœì†Œí™” (ì„±ëŠ¥ ìµœì í™”)
        try:
            best_proxy = self.db.get_best_proxy()
            if best_proxy:
                self.current_proxy = best_proxy
                return {"http://": self.current_proxy, "https://": self.current_proxy}
        except:
            # DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            pass
        
        # ì—†ìœ¼ë©´ ëœë¤ ì„ íƒ
        self.current_proxy = random.choice(self.proxies)
        return {"http://": self.current_proxy, "https://": self.current_proxy}
    
    def _get_headers(self) -> Dict[str, str]:
        """ìš”ì²­ í—¤ë” ìƒì„±"""
        headers = {
            "User-Agent": self._get_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        }
        return headers
    
    async def _random_delay(self, min_seconds: float = 0.5, max_seconds: float = 1.5):
        """ëœë¤ ì§€ì—° ì‹œê°„ (ì¸ê°„ì²˜ëŸ¼ ë³´ì´ê²Œ) - ìµœì í™”: ì§€ì—° ì‹œê°„ ë‹¨ì¶•"""
        delay = random.uniform(min_seconds, max_seconds)
        await asyncio.sleep(delay)
    
    async def _make_request(
        self,
        url: str,
        retry_count: int = 0
    ) -> httpx.Response:
        """
        HTTP ìš”ì²­ ìˆ˜í–‰ (ì¬ì‹œë„ ë° ìš°íšŒ ê¸°ëŠ¥ í¬í•¨)
        
        Args:
            url: ìš”ì²­í•  URL
            retry_count: í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            HTTP ì‘ë‹µ ê°ì²´
        """
        start_time = time.time()
        proxy_config = self._get_proxy()
        headers = self._get_headers()
        
        # ì§€ì—° ì‹œê°„ ì¶”ê°€ (ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€) - ìµœì í™”: ì§€ì—° ì‹œê°„ ë‹¨ì¶•
        if retry_count == 0:
            await self._random_delay(0.5, 1.5)  # 1-3ì´ˆ -> 0.5-1.5ì´ˆ
        else:
            # ì¬ì‹œë„ ì‹œ ë” ê¸´ ì§€ì—°
            await asyncio.sleep(self.retry_delay_base * (2 ** retry_count))
        
        try:
            # httpx.AsyncClient ì„¤ì •
            # httpx 0.25.2ì—ì„œëŠ” proxiesë¥¼ ì§€ì›í•˜ì§€ë§Œ, Noneì¼ ë•ŒëŠ” ì „ë‹¬í•˜ì§€ ì•ŠìŒ
            client_kwargs = {
                "timeout": self.timeout,
                "follow_redirects": True,
                "cookies": self.session_cookies
            }
            
            # í”„ë¡ì‹œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€ (Noneì´ ì•„ë‹ ë•Œë§Œ)
            if proxy_config is not None and proxy_config:
                # httpx ë²„ì „ì— ë”°ë¼ proxies ì§€ì› ì—¬ë¶€ í™•ì¸ (ëª¨ë“ˆ ë¡œë“œ ì‹œ 1íšŒ íŒë³„)
                if _HTTPX_SUPPORTS_PROXIES_PARAM:
                    # httpx 0.25.2 ì´í•˜: proxies ì¸ì ì§€ì›
                    client_kwargs["proxies"] = proxy_config
                else:
                    # httpx 0.26.0 ì´ìƒ: transport ì‚¬ìš©
                    from httpx import AsyncHTTPTransport
                    proxy_url = proxy_config.get("http://") or proxy_config.get("https://")
                    if proxy_url:
                        transport = AsyncHTTPTransport(proxy=proxy_url)
                        client_kwargs["transport"] = transport
            
            async with httpx.AsyncClient(**client_kwargs) as client:
                response = await client.get(url, headers=headers)
                response_time = time.time() - start_time
                
                # ì„±ê³µ ê¸°ë¡
                self.db.record_crawling_performance(
                    url=url,
                    success=True,
                    response_time=response_time,
                    status_code=response.status_code,
                    user_agent=self.current_user_agent,
                    proxy_used=self.current_proxy,
                    retry_count=retry_count
                )
                
                # ì¿ í‚¤ ì—…ë°ì´íŠ¸
                self.session_cookies.update(response.cookies)
                
                return response
                
        except httpx.HTTPStatusError as e:
            response_time = time.time() - start_time
            status_code = e.response.status_code if e.response else None
            
            # ì‹¤íŒ¨ ê¸°ë¡
            self.db.record_crawling_performance(
                url=url,
                success=False,
                response_time=response_time,
                status_code=status_code,
                error_message=str(e),
                user_agent=self.current_user_agent,
                proxy_used=self.current_proxy,
                retry_count=retry_count
            )
            
            # 429 (Too Many Requests) ë˜ëŠ” 403 (Forbidden)ì¸ ê²½ìš° ì¬ì‹œë„
            if status_code in [429, 403, 503] and retry_count < self.max_retries:
                # ë‹¤ë¥¸ User-Agentì™€ í”„ë¡ì‹œë¡œ ì¬ì‹œë„
                self.current_user_agent = None
                self.current_proxy = None
                return await self._make_request(url, retry_count + 1)
            
            raise
            
        except (httpx.RequestError, httpx.TimeoutException) as e:
            response_time = time.time() - start_time
            
            # ì‹¤íŒ¨ ê¸°ë¡
            self.db.record_crawling_performance(
                url=url,
                success=False,
                response_time=response_time,
                error_message=str(e),
                user_agent=self.current_user_agent,
                proxy_used=self.current_proxy,
                retry_count=retry_count
            )
            
            # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì¸ ê²½ìš° ì¬ì‹œë„
            if retry_count < self.max_retries:
                # ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„
                self.current_proxy = None
                return await self._make_request(url, retry_count + 1)
            
            raise
    
    
    async def crawl_product_with_playwright(self, url: str) -> Dict[str, Any]:
        """
        Playwrightë¥¼ ì‚¬ìš©í•œ ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§ (JavaScript ì‹¤í–‰ í™˜ê²½)
        
        Args:
            url: Qoo10 ìƒí’ˆ URL
            
        Returns:
            ìƒí’ˆ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        import logging
        logger = logging.getLogger(__name__)
        
        if not PLAYWRIGHT_AVAILABLE:
            raise Exception("Playwright is not available. Please install it: pip install playwright && playwright install")
        
        browser = None
        page = None
        playwright = None
        
        try:
            # URL ì •ê·œí™”
            normalized_url = self._normalize_product_url(url)
            logger.info(f"Playwright crawling product - URL: {normalized_url}")
            
            # Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™”
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-blink-features=AutomationControlled']
            )
            
            # ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent=self._get_user_agent(),
                locale='ja-JP',
                timezone_id='Asia/Tokyo'
            )
            
            page = await context.new_page()
            
            # í˜ì´ì§€ ë¡œë“œ
            logger.debug(f"Loading page: {normalized_url}")
            # networkidle ëŒ€ì‹  load ì‚¬ìš© (ë” ì•ˆì •ì ì´ê³  ë¹ ë¦„)
            # load: ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ, domcontentloadedë³´ë‹¤ ì•ˆì •ì 
            try:
                await page.goto(normalized_url, wait_until='load', timeout=60000)
            except PlaywrightTimeoutError:
                # load íƒ€ì„ì•„ì›ƒ ì‹œ domcontentloadedë¡œ ì¬ì‹œë„ (ë” ë¹ ë¦„)
                logger.warning(f"Page load timeout, trying domcontentloaded...")
                await page.goto(normalized_url, wait_until='domcontentloaded', timeout=30000)
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            await asyncio.sleep(2)
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ì§€ì—° ë¡œë”©ëœ ì½˜í…ì¸  ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë³´í˜¸)
            try:
                await asyncio.wait_for(
                    page.evaluate("""
                        async () => {
                            await new Promise((resolve) => {
                                let totalHeight = 0;
                                const distance = 100;
                                const maxHeight = 5000;
                                const timer = setInterval(() => {
                                    const scrollHeight = document.body.scrollHeight;
                                    window.scrollBy(0, distance);
                                    totalHeight += distance;
                                    
                                    if(totalHeight >= scrollHeight || totalHeight > maxHeight){
                                        clearInterval(timer);
                                        resolve();
                                    }
                                }, 100);
                            });
                        }
                    """),
                    timeout=10.0  # ìŠ¤í¬ë¡¤ ìµœëŒ€ 10ì´ˆ
                )
            except asyncio.TimeoutError:
                logger.warning("Scroll timeout, continuing with current content...")
            
            # ì¶”ê°€ ëŒ€ê¸°
            await asyncio.sleep(1)
            
            # HTML ê°€ì ¸ì˜¤ê¸°
            html_content = await page.content()
            soup = BeautifulSoup(html_content, 'lxml')
            
            # ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë©”ì„œë“œ ì¬ì‚¬ìš©)
            product_code = self._extract_product_code(normalized_url, soup)
            product_name = self._extract_product_name(soup)
            
            product_data = {
                "url": normalized_url,
                "product_code": product_code,
                "product_name": product_name,
                "crawled_with": "playwright"  # í¬ë¡¤ë§ ë°©ë²• í‘œì‹œ
            }
            
            # ê° í•„ë“œ ì¶”ì¶œ
            try:
                product_data["category"] = self._extract_category(soup)
            except Exception as e:
                logger.warning(f"Failed to extract category: {str(e)}")
                product_data["category"] = None
            
            try:
                product_data["brand"] = self._extract_brand(soup)
            except Exception as e:
                logger.warning(f"Failed to extract brand: {str(e)}")
                product_data["brand"] = None
            
            try:
                product_data["price"] = self._extract_price(soup)
            except Exception as e:
                logger.warning(f"Failed to extract price: {str(e)}")
                product_data["price"] = {}
            
            try:
                product_data["images"] = self._extract_images(soup)
            except Exception as e:
                logger.warning(f"Failed to extract images: {str(e)}")
                product_data["images"] = {}
            
            try:
                product_data["description"] = self._extract_description(soup)
            except Exception as e:
                logger.warning(f"Failed to extract description: {str(e)}")
                product_data["description"] = ""
            
            try:
                product_data["search_keywords"] = self._extract_search_keywords(soup)
            except Exception as e:
                logger.warning(f"Failed to extract search keywords: {str(e)}")
                product_data["search_keywords"] = []
            
            try:
                product_data["reviews"] = self._extract_reviews(soup)
            except Exception as e:
                logger.warning(f"Failed to extract reviews: {str(e)}")
                product_data["reviews"] = {}
            
            try:
                product_data["qna"] = self._extract_qna(soup)
            except Exception as e:
                logger.warning(f"Failed to extract Q&A: {str(e)}")
                product_data["qna"] = {}
            
            try:
                product_data["goods_info"] = self._extract_goods_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract goods info: {str(e)}")
                product_data["goods_info"] = {}
            
            try:
                product_data["seller_info"] = self._extract_seller_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract seller info: {str(e)}")
                product_data["seller_info"] = {}
            
            try:
                product_data["shipping_info"] = self._extract_shipping_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract shipping info: {str(e)}")
                product_data["shipping_info"] = {}
            
            try:
                product_data["is_move_product"] = self._extract_move_product(soup)
            except Exception as e:
                logger.warning(f"Failed to extract MOVE product flag: {str(e)}")
                product_data["is_move_product"] = False
            
            try:
                product_data["qpoint_info"] = self._extract_qpoint_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract Qpoint info: {str(e)}")
                product_data["qpoint_info"] = {}
            
            try:
                product_data["coupon_info"] = self._extract_coupon_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract coupon info: {str(e)}")
                product_data["coupon_info"] = {}
            
            try:
                product_data["page_structure"] = self._extract_page_structure(soup)
            except Exception as e:
                logger.warning(f"Failed to extract page structure: {str(e)}")
                product_data["page_structure"] = {}
            
            # ì¶”ê°€: JavaScriptë¡œ ë™ì  ë¡œë“œëœ ë°ì´í„° ì§ì ‘ ì¶”ì¶œ ì‹œë„
            try:
                # í˜ì´ì§€ì—ì„œ ì§ì ‘ ë°ì´í„° ì¶”ì¶œ
                js_data = await page.evaluate("""
                    () => {
                        const data = {};
                        
                        // ìƒí’ˆëª… (ê°€ê²© ì•ˆë‚´ í…ìŠ¤íŠ¸ ì œì™¸)
                        const excludePatterns = ['å…¨å‰²å¼•é©ç”¨å¾Œã®ä¾¡æ ¼æ¡ˆå†…', 'ä¾¡æ ¼æ¡ˆå†…', 'å‰²å¼•.*é©ç”¨', 'ã‚¯ãƒ¼ãƒãƒ³.*å‰²å¼•'];
                        const h1Elements = document.querySelectorAll('h1');
                        for (let h1 of h1Elements) {
                            const text = h1.textContent.trim();
                            let excluded = false;
                            for (let pattern of excludePatterns) {
                                if (new RegExp(pattern).test(text)) {
                                    excluded = true;
                                    break;
                                }
                            }
                            if (!excluded && text.length > 10 && text !== 'Qoo10' && text !== 'ãƒ›ãƒ¼ãƒ ') {
                                // ëŒ€ê´„í˜¸ ì œê±°
                                text = text.replace(/^\[.*?\]\s*/, '').replace(/\s*\[.*?\]$/, '').replace(/\[.*?\]/g, '').trim();
                                if (text.length > 10) {
                                    data.product_name = text;
                                    break;
                                }
                            }
                        }
                        
                        // title íƒœê·¸ì—ì„œ ìƒí’ˆëª… ì¶”ì¶œ (fallback)
                        if (!data.product_name) {
                            const title = document.querySelector('title');
                            if (title) {
                                let titleText = title.textContent.trim();
                                if (titleText.includes('|')) {
                                    titleText = titleText.split('|')[0].trim();
                                }
                                titleText = titleText.replace(/Qoo10/g, '').trim();
                                // ëŒ€ê´„í˜¸ ì œê±°
                                titleText = titleText.replace(/^\[.*?\]\s*/, '').replace(/\s*\[.*?\]$/, '').replace(/\[.*?\]/g, '').trim();
                                if (titleText.length > 3) {
                                    data.product_name = titleText;
                                }
                            }
                        }
                        
                        // ê°€ê²© ì •ë³´ (ìœ íš¨ì„± ê²€ì¦ í¬í•¨)
                        const priceElements = document.querySelectorAll('[class*="price"], [class*="prc"]');
                        const prices = [];
                        priceElements.forEach(el => {
                            const text = el.textContent.trim();
                            const match = text.match(/(\\d{1,3}(?:,\\d{3})*)å††/);
                            if (match) {
                                const price = parseInt(match[1].replace(/,/g, ''));
                                // í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ (100~1,000,000ì—”)
                                if (price >= 100 && price <= 1000000) {
                                    prices.push(price);
                                }
                            }
                        });
                        data.prices = prices;
                        
                        // ë¦¬ë·° ìˆ˜ (ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„)
                        const reviewPatterns = [
                            /ãƒ¬ãƒ“ãƒ¥ãƒ¼\\s*\\((\\d+)\\)/,
                            /è©•ä¾¡\\s*\\((\\d+)\\)/,
                            /(\\d+)\\s*ãƒ¬ãƒ“ãƒ¥ãƒ¼/,
                            /(\\d+)\\s*è©•ä¾¡/
                        ];
                        for (let pattern of reviewPatterns) {
                            const match = document.body.textContent.match(pattern);
                            if (match) {
                                data.review_count = parseInt(match[1]);
                                break;
                            }
                        }
                        
                        // í‰ì 
                        const ratingMatch = document.body.textContent.match(/(\\d+\\.?\\d*)\\s*\\((\\d+)\\)/);
                        if (ratingMatch) {
                            data.rating = parseFloat(ratingMatch[1]);
                            if (!data.review_count) {
                            data.review_count = parseInt(ratingMatch[2]);
                            }
                        }
                        
                        // Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ì¶”ì¶œ
                        const qpointText = document.body.textContent;
                        const receiveMatch = qpointText.match(/å—å–ç¢ºèª[ï¼š:\\s]*æœ€å¤§?\\s*(\\d+)P/i);
                        if (receiveMatch) data.receive_confirmation_points = parseInt(receiveMatch[1]);
                        
                        const reviewPointMatch = qpointText.match(/ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ[ï¼š:\\s]*æœ€å¤§?\\s*(\\d+)P/i);
                        if (reviewPointMatch) data.review_points = parseInt(reviewPointMatch[1]);
                        
                        const maxPointMatch = qpointText.match(/æœ€å¤§\\s*(\\d+)P/i);
                        if (maxPointMatch) data.max_points = parseInt(maxPointMatch[1]);
                        
                        return data;
                    }
                """)
                
                # JavaScriptì—ì„œ ì¶”ì¶œí•œ ë°ì´í„° ë³‘í•©
                if js_data.get('product_name') and (not product_data.get('product_name') or product_data.get('product_name') == 'ìƒí’ˆëª… ì—†ìŒ'):
                    # ê°€ê²© ì•ˆë‚´ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œì§€ í™•ì¸
                    exclude_patterns = ['å…¨å‰²å¼•é©ç”¨å¾Œã®ä¾¡æ ¼æ¡ˆå†…', 'ä¾¡æ ¼æ¡ˆå†…']
                    product_name = js_data['product_name']
                    excluded = False
                    for pattern in exclude_patterns:
                        if pattern in product_name:
                            excluded = True
                            break
                    if not excluded:
                        # ìƒí’ˆëª… ì •ì œ: ëŒ€ê´„í˜¸ ì œê±°
                        product_name = re.sub(r'^\[.*?\]\s*', '', product_name)  # ì•ìª½ ëŒ€ê´„í˜¸ ì œê±°
                        product_name = re.sub(r'\s*\[.*?\]$', '', product_name)  # ë’¤ìª½ ëŒ€ê´„í˜¸ ì œê±°
                        product_name = re.sub(r'\[.*?\]', '', product_name)  # ì¤‘ê°„ ëŒ€ê´„í˜¸ ì œê±°
                        product_name = ' '.join(product_name.split())  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
                        product_data['product_name'] = product_name
                
                if js_data.get('prices'):
                    if not product_data.get('price', {}).get('sale_price'):
                        # ìœ íš¨í•œ ê°€ê²©ë§Œ ì‚¬ìš© (ì´ë¯¸ JavaScriptì—ì„œ í•„í„°ë§ë¨)
                        prices = js_data['prices']
                        if prices:
                            # ì—¬ëŸ¬ ê°€ê²© ì¤‘ ìµœì†Œê°’ì„ íŒë§¤ê°€ë¡œ ì¶”ì •
                            product_data.setdefault('price', {})['sale_price'] = min(prices)
                
                if js_data.get('review_count') and not product_data.get('reviews', {}).get('review_count'):
                    product_data.setdefault('reviews', {})['review_count'] = js_data['review_count']
                
                if js_data.get('rating') and not product_data.get('reviews', {}).get('rating'):
                    product_data.setdefault('reviews', {})['rating'] = js_data['rating']
                
                # Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ë³‘í•©
                if js_data.get('receive_confirmation_points'):
                    product_data.setdefault('qpoint_info', {})['receive_confirmation_points'] = js_data['receive_confirmation_points']
                if js_data.get('review_points'):
                    product_data.setdefault('qpoint_info', {})['review_points'] = js_data['review_points']
                if js_data.get('max_points'):
                    product_data.setdefault('qpoint_info', {})['max_points'] = js_data['max_points']
                
                # Shop ì •ë³´ ì¶”ì¶œ (ìƒí’ˆ í˜ì´ì§€ì—ì„œ ê°€ëŠ¥í•œ ì •ë³´ë§Œ)
                shop_js_data = await page.evaluate("""
                    () => {
                        const data = {};
                        
                        // íŒ”ë¡œì›Œ ìˆ˜
                        const followerMatch = document.body.textContent.match(/ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼[_\s]*(\d{1,3}(?:,\d{3})*)/);
                        if (followerMatch) {
                            data.follower_count = parseInt(followerMatch[1].replace(/,/g, ''));
                        }
                        
                        return data;
                    }
                """)
                
                if shop_js_data.get('follower_count'):
                    if not product_data.get("seller_info"):
                        product_data["seller_info"] = {}
                    product_data["seller_info"]["follower_count"] = shop_js_data['follower_count']
                    
            except Exception as e:
                logger.warning(f"Failed to extract JS data: {str(e)}")
            
            # API êµ¬ì¡° ê¸°ë°˜ ë°ì´í„° ì •ê·œí™” (API Key ì—†ì´ë„ êµ¬ì¡° ì°¸ê³ )
            if API_SCHEMA_AVAILABLE and Qoo10APISchema:
                try:
                    # í¬ë¡¤ëŸ¬ ë°ì´í„°ë¥¼ API êµ¬ì¡°ì— ë§ê²Œ ì •ê·œí™”
                    normalized_data = Qoo10APISchema.normalize_crawler_data_to_api_structure(product_data)
                    
                    # ì •ê·œí™”ëœ ë°ì´í„°ì˜ ìœ íš¨í•œ í•„ë“œë§Œ í¬ë¡¤ëŸ¬ ë°ì´í„°ì— ë°˜ì˜
                    # (ëˆ„ë½ëœ í•„ë“œ ë³´ì™„, íƒ€ì… ë³€í™˜, ê²€ì¦ í†µê³¼í•œ ê°’ë§Œ ì‚¬ìš©)
                    product_data = self._apply_api_schema_normalization(product_data, normalized_data)
                    
                    logger.info("API êµ¬ì¡° ê¸°ë°˜ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ (Playwright)")
                except Exception as e:
                    logger.warning(f"API êµ¬ì¡° ê¸°ë°˜ ì •ê·œí™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(e)}")
            
            logger.info(f"Playwright crawling completed - Product: {product_name or 'Unknown'}, Code: {product_code or 'N/A'}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            try:
                self.db.save_crawled_product(product_data)
            except Exception as e:
                logger.warning(f"Failed to save to database: {str(e)}")
            
            # ì„ë² ë”© ì €ì¥ (ìë™ í•™ìŠµ)
            try:
                auto_learn = os.getenv("EMBEDDING_AUTO_LEARN", "1").lower() in {"1", "true", "yes"}
                if auto_learn:
                    from .embedding_integration import EmbeddingIntegration
                    embedding_integration = EmbeddingIntegration(db=self.db)
                    embedding_integration.save_crawled_texts(product_data, normalized_url, auto_learn=True)
            except Exception as e:
                logger.warning(f"Failed to save embeddings: {str(e)}")
            
            return product_data
        
        except PlaywrightTimeoutError as e:
            error_msg = f"Playwright timeout error: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"Error in Playwright crawling: {str(e)}"
            logger.error(error_msg, exc_info=True)
            raise Exception(error_msg)
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if page:
                try:
                    await page.close()
                except:
                    pass
            if browser:
                try:
                    await browser.close()
                except:
                    pass
            if playwright:
                try:
                    await playwright.stop()
                except:
                    pass
    
    async def crawl_product(self, url: str, use_playwright: bool = False) -> Dict[str, Any]:
        """
        ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§ (ë‹¤ì–‘í•œ URL í˜•ì‹ ì§€ì›)
        
        Args:
            url: Qoo10 ìƒí’ˆ URL (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›: /g/XXXXX, /item/.../XXXXX, ?goodscode=XXXXX ë“±)
            use_playwright: Trueì´ë©´ Playwright ì‚¬ìš©, Falseì´ë©´ ê¸°ë³¸ HTTP í¬ë¡¤ë§ (ê¸°ë³¸ê°’: False)
            
        Returns:
            ìƒí’ˆ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        import logging
        logger = logging.getLogger(__name__)
        
        # ìš°ì„  í¬ë¡¤ë§ í•„ë“œ ë¡œë“œ (ì˜¤ë¥˜ ì‹ ê³ ëœ í•„ë“œ)
        if self.error_reporting_service:
            try:
                self._priority_fields = self.error_reporting_service.get_priority_fields_for_crawling()
                # ê° ìš°ì„  í•„ë“œì— ëŒ€í•œ Chunk ì •ë³´ ë¡œë“œ
                for field_name in self._priority_fields:
                    chunks = self.error_reporting_service.get_chunks_for_field(field_name)
                    if chunks:
                        self._priority_chunks[field_name] = chunks
                logger.info(f"Priority fields loaded: {self._priority_fields}")
            except Exception as e:
                logger.warning(f"Failed to load priority fields: {str(e)}")
                self._priority_fields = []
        
        # Playwright ì‚¬ìš© ìš”ì²­ ì‹œ
        if use_playwright:
            if PLAYWRIGHT_AVAILABLE:
                return await self.crawl_product_with_playwright(url)
            else:
                logger.warning("Playwright not available, falling back to HTTP crawling")
        
        # ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ (API ì‚¬ìš©ì„ ìœ„í•´)
        product_code = self._extract_product_code_from_url(url)
        api_data = None
        
        # Qoo10 APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¡°íšŒ ì‹œë„ (ìš°ì„ ìˆœìœ„)
        if self.api_service and product_code:
            try:
                logger.info(f"Qoo10 APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆ ì •ë³´ ì¡°íšŒ ì‹œë„: {product_code}")
                api_data = await self.api_service.fetch_product_data(product_code, use_api=True)
                if api_data:
                    logger.info(f"Qoo10 APIë¡œ ìƒí’ˆ ì •ë³´ ì¡°íšŒ ì„±ê³µ: {product_code}")
                    # API ë°ì´í„°ì— URL ì¶”ê°€
                    api_data["url"] = url
                    api_data["product_code"] = product_code
                    return api_data
                else:
                    logger.info(f"Qoo10 APIë¡œ ìƒí’ˆ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨, í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜: {product_code}")
            except Exception as e:
                logger.warning(f"Qoo10 API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜: {str(e)}")
        
        try:
            # URL ì •ê·œí™” (ë‹¤ì–‘í•œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì‹œë„)
            normalized_url = self._normalize_product_url(url)
            logger.info(f"Crawling product - Original URL: {url}, Normalized URL: {normalized_url}")
            
            # HTTP ìš”ì²­ (ì •ê·œí™”ëœ URL ì‚¬ìš©)
            logger.debug(f"Making HTTP request to: {normalized_url}")
            response = await self._make_request(normalized_url)
            response.raise_for_status()
            logger.debug(f"HTTP response status: {response.status_code}, Content length: {len(response.text)}")
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ìƒí’ˆ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (AI í•™ìŠµ ê¸°ë°˜ ì„ íƒì ì‚¬ìš©)
            # í˜ì´ì§€ êµ¬ì¡° ì¶”ì¶œì€ ì„ íƒì ìœ¼ë¡œ ìˆ˜í–‰ (ì„±ëŠ¥ ìµœì í™”)
            # ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ë¥¼ ìœ„í•´ ë¨¼ì € í˜ì´ì§€ êµ¬ì¡° ì¶”ì¶œ
            page_structure = None
            is_error_page = False
            error_indicators = []
            try:
                page_structure = self._extract_page_structure(soup)
                # ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ í™•ì¸
                if page_structure and page_structure.get("is_error_page", False):
                    is_error_page = True
                    error_indicators = page_structure.get("error_indicators", [])
                    logger.warning(f"âš ï¸ ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ë¨ (HTTP í¬ë¡¤ë§) - ì§€í‘œ: {error_indicators}")
            except Exception as e:
                logger.warning(f"Failed to extract page structure: {str(e)}")
                # í˜ì´ì§€ êµ¬ì¡° ì¶”ì¶œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                pass
            
            # ì—ëŸ¬ í˜ì´ì§€ê°€ ê°ì§€ë˜ê³  Playwrightê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìë™ ì¬ì‹œë„
            if is_error_page:
                logger.warning(f"âš ï¸ ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ë¨ (HTTP í¬ë¡¤ë§) - ì§€í‘œ: {error_indicators}")
                logger.warning(f"âš ï¸ HTML ê¸¸ì´: {len(response.text)} bytes, Playwright ì‚¬ìš© ê°€ëŠ¥: {PLAYWRIGHT_AVAILABLE}, use_playwright: {use_playwright}")
                
                if PLAYWRIGHT_AVAILABLE and not use_playwright:
                    logger.info("ğŸ”„ Playwrightê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. Playwrightë¡œ ìë™ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    try:
                        playwright_result = await self.crawl_product_with_playwright(url)
                        # Playwright ê²°ê³¼ì— ì¬ì‹œë„ ì •ë³´ ì¶”ê°€
                        playwright_result["_retry_info"] = {
                            "original_method": "httpx",
                            "retry_method": "playwright",
                            "retry_reason": "error_page_detected",
                            "error_indicators": error_indicators
                        }
                        # Playwrightë¡œ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§í•œ ê²½ìš°, ì—ëŸ¬ í˜ì´ì§€ ì •ë³´ ì œê±°
                        if playwright_result.get("page_structure"):
                            playwright_result["page_structure"].pop("is_error_page", None)
                            playwright_result["page_structure"].pop("error_indicators", None)
                        extracted_name = playwright_result.get('product_name', 'Unknown')
                        logger.info(f"âœ… Playwright ì¬ì‹œë„ ì„±ê³µ - ìƒí’ˆëª…: {extracted_name}")
                        if extracted_name and extracted_name != "ìƒí’ˆëª… ì—†ìŒ" and extracted_name != "Unknown":
                            logger.info(f"âœ… ìƒí’ˆëª… ì¶”ì¶œ ì„±ê³µ: {extracted_name}")
                        return playwright_result
                    except Exception as e:
                        logger.error(f"âŒ Playwright ì¬ì‹œë„ ì‹¤íŒ¨: {str(e)}", exc_info=True)
                        logger.warning("HTTP í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì—ëŸ¬ í˜ì´ì§€ì´ë¯€ë¡œ ë°ì´í„°ê°€ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        # Playwright ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ HTTP í¬ë¡¤ë§ ê²°ê³¼ ê³„ì† ì‚¬ìš© (ì—ëŸ¬ í˜ì´ì§€ ì •ë³´ í¬í•¨)
                else:
                    if not PLAYWRIGHT_AVAILABLE:
                        logger.warning("âš ï¸ Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì¬ì‹œë„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install playwright && playwright install ì‹¤í–‰ í•„ìš”")
                    elif use_playwright:
                        logger.info("ì´ë¯¸ Playwrightë¥¼ ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ ì¬ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # ê° í•„ë“œ ì¶”ì¶œ ì‹œë„ (ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ê°’ ì‚¬ìš©)
            product_code = None
            try:
                product_code = self._extract_product_code(normalized_url, soup)
            except Exception as e:
                logger.warning(f"Failed to extract product code: {str(e)}")
            
            product_name = None
            try:
                product_name = self._extract_product_name(soup)
            except Exception as e:
                logger.warning(f"Failed to extract product name: {str(e)}")
            
            # ë‚˜ë¨¸ì§€ í•„ë“œë“¤ë„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            product_data = {
                "url": normalized_url,  # ì •ê·œí™”ëœ URL ì‚¬ìš©
                "product_code": product_code,
                "product_name": product_name,
                "crawled_with": "httpx"  # í¬ë¡¤ë§ ë°©ë²• í‘œì‹œ
            }
            
            # ê° í•„ë“œë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            try:
                product_data["category"] = self._extract_category(soup)
            except Exception as e:
                logger.warning(f"Failed to extract category: {str(e)}")
                product_data["category"] = None
            
            try:
                product_data["brand"] = self._extract_brand(soup)
            except Exception as e:
                logger.warning(f"Failed to extract brand: {str(e)}")
                product_data["brand"] = None
            
            try:
                product_data["price"] = self._extract_price(soup)
            except Exception as e:
                logger.warning(f"Failed to extract price: {str(e)}")
                product_data["price"] = {}
            
            try:
                product_data["images"] = self._extract_images(soup)
            except Exception as e:
                logger.warning(f"Failed to extract images: {str(e)}")
                product_data["images"] = {}
            
            try:
                product_data["description"] = self._extract_description(soup)
            except Exception as e:
                logger.warning(f"Failed to extract description: {str(e)}")
                product_data["description"] = ""
            
            try:
                product_data["search_keywords"] = self._extract_search_keywords(soup)
            except Exception as e:
                logger.warning(f"Failed to extract search keywords: {str(e)}")
                product_data["search_keywords"] = []
            
            try:
                product_data["reviews"] = self._extract_reviews(soup)
            except Exception as e:
                logger.warning(f"Failed to extract reviews: {str(e)}")
                product_data["reviews"] = {}
            
            try:
                product_data["qna"] = self._extract_qna(soup)
            except Exception as e:
                logger.warning(f"Failed to extract Q&A: {str(e)}")
                product_data["qna"] = {}
            
            try:
                product_data["goods_info"] = self._extract_goods_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract goods info: {str(e)}")
                product_data["goods_info"] = {}
            
            try:
                product_data["seller_info"] = self._extract_seller_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract seller info: {str(e)}")
                product_data["seller_info"] = {}
            
            try:
                product_data["shipping_info"] = self._extract_shipping_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract shipping info: {str(e)}")
                product_data["shipping_info"] = {}
            
            try:
                product_data["is_move_product"] = self._extract_move_product(soup)
            except Exception as e:
                logger.warning(f"Failed to extract MOVE product flag: {str(e)}")
                product_data["is_move_product"] = False
            
            try:
                product_data["qpoint_info"] = self._extract_qpoint_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract Qpoint info: {str(e)}")
                product_data["qpoint_info"] = {}
            
            try:
                product_data["coupon_info"] = self._extract_coupon_info(soup)
            except Exception as e:
                logger.warning(f"Failed to extract coupon info: {str(e)}")
                product_data["coupon_info"] = {}
            
            product_data["page_structure"] = page_structure  # í˜ì´ì§€ êµ¬ì¡° ë° ëª¨ë“  div class ì •ë³´ ì¶”ê°€
            
            # API êµ¬ì¡° ê¸°ë°˜ ë°ì´í„° ì •ê·œí™” (API Key ì—†ì´ë„ êµ¬ì¡° ì°¸ê³ )
            if API_SCHEMA_AVAILABLE and Qoo10APISchema:
                try:
                    # í¬ë¡¤ëŸ¬ ë°ì´í„°ë¥¼ API êµ¬ì¡°ì— ë§ê²Œ ì •ê·œí™”
                    normalized_data = Qoo10APISchema.normalize_crawler_data_to_api_structure(product_data)
                    
                    # ì •ê·œí™”ëœ ë°ì´í„°ì˜ ìœ íš¨í•œ í•„ë“œë§Œ í¬ë¡¤ëŸ¬ ë°ì´í„°ì— ë°˜ì˜
                    # (ëˆ„ë½ëœ í•„ë“œ ë³´ì™„, íƒ€ì… ë³€í™˜, ê²€ì¦ í†µê³¼í•œ ê°’ë§Œ ì‚¬ìš©)
                    product_data = self._apply_api_schema_normalization(product_data, normalized_data)
                    
                    logger.info("API êµ¬ì¡° ê¸°ë°˜ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"API êµ¬ì¡° ê¸°ë°˜ ì •ê·œí™” ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(e)}")
            
            logger.info(f"Crawling completed - Product: {product_name or 'Unknown'}, Code: {product_code or 'N/A'}")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
            # ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¶„ì„ì€ ê³„ì† ì§„í–‰
            try:
                self.db.save_crawled_product(product_data)
            except Exception as e:
                logger.warning(f"Failed to save to database: {str(e)}")
                pass  # ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
            
            # ì„ë² ë”© ì €ì¥ (ìë™ í•™ìŠµ)
            try:
                auto_learn = os.getenv("EMBEDDING_AUTO_LEARN", "1").lower() in {"1", "true", "yes"}
                if auto_learn:
                    from .embedding_integration import EmbeddingIntegration
                    embedding_integration = EmbeddingIntegration(db=self.db)
                    embedding_integration.save_crawled_texts(product_data, url, auto_learn=True)
            except Exception as e:
                logger.warning(f"Failed to save embeddings: {str(e)}")
            
            return product_data
        
        except httpx.HTTPStatusError as e:
            error_msg = f"HTTP {e.response.status_code} error while crawling: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
        except httpx.HTTPError as e:
            error_msg = f"HTTP error while crawling: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"Error crawling product: {str(e)}"
            logger.error(error_msg, exc_info=True)
            raise Exception(error_msg)
    
    
    def _extract_with_learning(
        self,
        selector_type: str,
        soup: BeautifulSoup,
        default_selectors: List[str],
        extract_func
    ) -> Any:
        """
        AI í•™ìŠµ ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ - ìµœì í™”: DB ì¡°íšŒ ìµœì†Œí™”, ìš°ì„  í¬ë¡¤ë§ Chunk í™œìš©
        
        Args:
            selector_type: ì„ íƒì íƒ€ì… ('product_name', 'price', etc.)
            soup: BeautifulSoup ê°ì²´
            default_selectors: ê¸°ë³¸ ì„ íƒì ëª©ë¡
            extract_func: ì¶”ì¶œ í•¨ìˆ˜
            
        Returns:
            ì¶”ì¶œëœ ë°ì´í„°
        """
        # ìš°ì„  í¬ë¡¤ë§ í•„ë“œì¸ ê²½ìš° Chunk ì •ë³´ì˜ ì„ íƒìë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì‹œë„
        field_name = selector_type  # selector_typeì´ í•„ë“œëª…ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
        if self._priority_fields and field_name in self._priority_fields:
            if field_name in self._priority_chunks:
                chunks = self._priority_chunks[field_name]
                logger = logging.getLogger(__name__)
                logger.info(f"Using priority chunks for field: {field_name}, chunks count: {len(chunks)}")
                
                # Chunk ì •ë³´ì—ì„œ ì„ íƒì íŒ¨í„´ ì¶”ì¶œí•˜ì—¬ ìš°ì„  ì‹œë„
                for chunk in chunks:
                    chunk_data = chunk.get("chunk_data", {})
                    selector_pattern = chunk.get("selector_pattern")
                    
                    # selector_patternì´ ìˆìœ¼ë©´ ìš°ì„  ì‹œë„
                    if selector_pattern:
                        try:
                            result = extract_func(soup, selector_pattern)
                            if result and result != "ìƒí’ˆëª… ì—†ìŒ" and result != "":
                                logger.info(f"Successfully extracted {field_name} using priority chunk selector: {selector_pattern}")
                                return result
                        except Exception as e:
                            logger.debug(f"Failed to extract using chunk selector {selector_pattern}: {str(e)}")
                            continue
                    
                    # chunk_dataì—ì„œ ê´€ë ¨ í´ë˜ìŠ¤ ì •ë³´ í™œìš©
                    related_classes = chunk_data.get("related_classes", [])
                    if related_classes:
                        # ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¥¼ ì„ íƒìë¡œ ì‚¬ìš©
                        for class_name in related_classes[:3]:  # ìƒìœ„ 3ê°œë§Œ ì‹œë„
                            selector = f".{class_name}"
                            try:
                                result = extract_func(soup, selector)
                                if result and result != "ìƒí’ˆëª… ì—†ìŒ" and result != "":
                                    logger.info(f"Successfully extracted {field_name} using priority chunk class: {class_name}")
                                    return result
                            except Exception:
                                continue
        
        # ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ì„ íƒìë¥¼ ë¨¼ì € ì‹œë„ (DB ì¡°íšŒ ì—†ì´)
        for selector in default_selectors[:5]:  # ìƒìœ„ 5ê°œë§Œ ë¨¼ì € ì‹œë„
            try:
                result = extract_func(soup, selector)
                if result and result != "ìƒí’ˆëª… ì—†ìŒ" and result != "":
                    # ì„±ê³µ ê¸°ë¡ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (ì„±ëŠ¥ ìµœì í™”)
                    try:
                        self.db.record_selector_performance(
                            selector_type=selector_type,
                            selector=selector,
                            success=True,
                            data_quality=1.0 if result else 0.0
                        )
                    except:
                        pass  # DB ê¸°ë¡ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
                    return result
            except Exception:
                continue
        
        # ê¸°ë³¸ ì„ íƒìë¡œ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ DB ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
        try:
            best_selectors = self.db.get_best_selectors(selector_type, limit=5)  # limit ê°ì†Œ: 10 -> 5
            if best_selectors:
                for selector_info in best_selectors:
                    selector = selector_info.get("selector")
                    if selector and selector not in default_selectors:
                        try:
                            result = extract_func(soup, selector)
                            if result and result != "ìƒí’ˆëª… ì—†ìŒ" and result != "":
                                return result
                        except:
                            continue
        except:
            # DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            pass
        
        # ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°
        return extract_func(soup, None) if extract_func else None
    
    def _normalize_product_url(self, url: str) -> str:
        """Qoo10 ìƒí’ˆ URL ì •ê·œí™” (ë‹¤ì–‘í•œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)"""
        import logging
        logger = logging.getLogger(__name__)
        
        # URLì—ì„œ ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ
        product_code = None
        original_url = url
        
        # ë‹¤ì–‘í•œ íŒ¨í„´ì—ì„œ ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ
        patterns = [
            (r'goodscode=(\d+)', 1),  # ?goodscode=123456
            (r'/g/(\d+)', 1),  # /g/123456
            (r'/item/[^/]+/(\d+)', 1),  # /item/.../123456
            (r'/item/[^/]+/(\d+)\?', 1),  # /item/.../123456?
            (r'#(\d+)$', 1),  # #123456 (ëì— ìˆëŠ” ê²½ìš°)
        ]
        
        for pattern, group in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                product_code = match.group(group)
                logger.debug(f"Extracted product code '{product_code}' from URL using pattern: {pattern}")
                break
        
        # ìƒí’ˆ ì½”ë“œê°€ ìˆìœ¼ë©´ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if product_code:
            # í‘œì¤€ í˜•ì‹: https://www.qoo10.jp/gmkt.inc/Goods/Goods.aspx?goodscode=XXXXX
            normalized = f"https://www.qoo10.jp/gmkt.inc/Goods/Goods.aspx?goodscode={product_code}"
            logger.info(f"Normalized URL: {original_url} -> {normalized}")
            return normalized
        
        # ë³€í™˜í•  ìˆ˜ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (ë¡œê·¸ ë‚¨ê¸°ê¸°)
        logger.warning(f"Could not extract product code from URL: {url}, using original URL")
        return url
    
    def _extract_product_code_from_url(self, url: str) -> Optional[str]:
        """URLì—ì„œ ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ (API ì‚¬ìš©ì„ ìœ„í•´)"""
        # ë‹¤ì–‘í•œ íŒ¨í„´ì—ì„œ ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ
        patterns = [
            r'goodscode=(\d+)',  # ?goodscode=123456
            r'/g/(\d+)',  # /g/123456
            r'/item/[^/]+/(\d+)',  # /item/.../123456
            r'/item/[^/]+/(\d+)\?',  # /item/.../123456?
            r'#(\d+)$',  # #123456 (ëì— ìˆëŠ” ê²½ìš°)
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None
    
    @staticmethod
    def _apply_api_schema_normalization(
        crawler_data: Dict[str, Any],
        normalized_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        API ìŠ¤í‚¤ë§ˆë¡œ ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ í¬ë¡¤ëŸ¬ ë°ì´í„°ì— ë°˜ì˜
        
        Args:
            crawler_data: ì›ë³¸ í¬ë¡¤ëŸ¬ ë°ì´í„°
            normalized_data: API êµ¬ì¡°ë¡œ ì •ê·œí™”ëœ ë°ì´í„°
            
        Returns:
            ì •ê·œí™”ê°€ ì ìš©ëœ í¬ë¡¤ëŸ¬ ë°ì´í„°
        """
        # ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ í¬ë¡¤ëŸ¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        from services.data_validator import DataValidator
        normalized_crawler_format = DataValidator._convert_api_structure_to_crawler_format(normalized_data)
        
        # í¬ë¡¤ëŸ¬ ë°ì´í„°ì— ì •ê·œí™”ëœ ê°’ ë°˜ì˜ (í¬ë¡¤ëŸ¬ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
        result = crawler_data.copy()
        
        # ê¸°ë³¸ ì •ë³´
        if not result.get("product_name") or result.get("product_name") == "ìƒí’ˆëª… ì—†ìŒ":
            if normalized_crawler_format.get("product_name"):
                result["product_name"] = normalized_crawler_format["product_name"]
        
        if not result.get("product_code"):
            if normalized_crawler_format.get("product_code"):
                result["product_code"] = normalized_crawler_format["product_code"]
        
        if not result.get("category"):
            if normalized_crawler_format.get("category"):
                result["category"] = normalized_crawler_format["category"]
        
        if not result.get("brand"):
            if normalized_crawler_format.get("brand"):
                result["brand"] = normalized_crawler_format["brand"]
        
        # ê°€ê²© ì •ë³´ (í¬ë¡¤ëŸ¬ ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°ë§Œ)
        if not result.get("price", {}).get("sale_price") or not (100 <= result.get("price", {}).get("sale_price", 0) <= 1000000):
            if normalized_crawler_format.get("price", {}).get("sale_price"):
                if "price" not in result:
                    result["price"] = {}
                result["price"]["sale_price"] = normalized_crawler_format["price"]["sale_price"]
        
        if not result.get("price", {}).get("original_price"):
            if normalized_crawler_format.get("price", {}).get("original_price"):
                if "price" not in result:
                    result["price"] = {}
                result["price"]["original_price"] = normalized_crawler_format["price"]["original_price"]
        
        # ë¦¬ë·° ì •ë³´
        if not result.get("reviews", {}).get("review_count"):
            if normalized_crawler_format.get("reviews", {}).get("review_count"):
                if "reviews" not in result:
                    result["reviews"] = {}
                result["reviews"]["review_count"] = normalized_crawler_format["reviews"]["review_count"]
        
        if not result.get("reviews", {}).get("rating"):
            if normalized_crawler_format.get("reviews", {}).get("rating"):
                if "reviews" not in result:
                    result["reviews"] = {}
                result["reviews"]["rating"] = normalized_crawler_format["reviews"]["rating"]
        
        # ì´ë¯¸ì§€ ì •ë³´ (í¬ë¡¤ëŸ¬ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if not result.get("images", {}).get("detail_images"):
            if normalized_crawler_format.get("images", {}).get("detail_images"):
                if "images" not in result:
                    result["images"] = {}
                result["images"]["detail_images"] = normalized_crawler_format["images"]["detail_images"]
        
        # Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ (í¬ë¡¤ëŸ¬ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if not result.get("qpoint_info") or not any(result.get("qpoint_info", {}).values()):
            if normalized_crawler_format.get("qpoint_info") and any(normalized_crawler_format["qpoint_info"].values()):
                result["qpoint_info"] = normalized_crawler_format["qpoint_info"]
        
        # ì¿ í° ì •ë³´ (í¬ë¡¤ëŸ¬ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if not result.get("coupon_info", {}).get("has_coupon"):
            if normalized_crawler_format.get("coupon_info", {}).get("has_coupon"):
                result["coupon_info"] = normalized_crawler_format["coupon_info"]
        
        # ë°°ì†¡ ì •ë³´ (í¬ë¡¤ëŸ¬ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if not result.get("shipping_info"):
            if normalized_crawler_format.get("shipping_info"):
                result["shipping_info"] = normalized_crawler_format["shipping_info"]
        
        return result
    
    def _extract_product_code(self, url: str, soup: BeautifulSoup) -> Optional[str]:
        """ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ (ë‹¤ì–‘í•œ URL í˜•ì‹ ì§€ì›)"""
        # 1. URLì—ì„œ ì¶”ì¶œ ì‹œë„ - ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›
        patterns = [
            r'goodscode=(\d+)',  # ê¸°ë³¸ í˜•ì‹: ?goodscode=123456
            r'/g/(\d+)',  # ì§§ì€ í˜•ì‹: /g/123456
            r'/item/[^/]+/(\d+)',  # ê¸´ í˜•ì‹: /item/.../123456
            r'/item/[^/]+/(\d+)\?',  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° í¬í•¨
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                return match.group(1)
        
        # 2. í˜ì´ì§€ì—ì„œ ì¶”ì¶œ ì‹œë„
        code_elem = soup.find('input', {'name': 'goodscode'}) or soup.find('meta', {'property': 'product:retailer_item_id'})
        if code_elem:
            code = code_elem.get('value') or code_elem.get('content')
            if code:
                return code
        
        # 3. JSON-LD ìŠ¤í‚¤ë§ˆì—ì„œ ì¶”ì¶œ ì‹œë„
        json_ld = soup.find('script', {'type': 'application/ld+json'})
        if json_ld:
            try:
                import json
                data = json.loads(json_ld.string)
                if isinstance(data, dict):
                    # product:retailer_item_id ë˜ëŠ” sku ì°¾ê¸°
                    if 'sku' in data:
                        return str(data['sku'])
                    if 'productID' in data:
                        return str(data['productID'])
            except (ValueError, json.JSONDecodeError, TypeError):
                pass
        
        # 4. ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ ì‹œë„
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            prop = meta.get('property') or meta.get('name')
            if prop and ('product' in prop.lower() or 'item' in prop.lower()):
                content = meta.get('content')
                if content and content.isdigit():
                    return content
        
        return None
    
    def _extract_product_name(self, soup: BeautifulSoup) -> str:
        """ìƒí’ˆëª… ì¶”ì¶œ (AI í•™ìŠµ ê¸°ë°˜) - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„  ë° ì •í™•ë„ í–¥ìƒ"""
        # #region agent log
        _log_debug("debug-session", "run1", "A", "crawler.py:_extract_product_name", "ìƒí’ˆëª… ì¶”ì¶œ ì‹œì‘", {})
        # #endregion

        # ì œì™¸í•  í…ìŠ¤íŠ¸ íŒ¨í„´ (ê°€ê²© ì•ˆë‚´, ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ë“±)
        exclude_patterns = [
            r'å…¨å‰²å¼•é©ç”¨å¾Œã®ä¾¡æ ¼æ¡ˆå†…',  # ê°€ê²© ì•ˆë‚´ í…ìŠ¤íŠ¸
            r'ä¾¡æ ¼æ¡ˆå†…',  # ê°€ê²© ì•ˆë‚´
            r'å‰²å¼•.*é©ç”¨',  # í• ì¸ ì ìš© ê´€ë ¨
            r'ã‚¯ãƒ¼ãƒãƒ³.*å‰²å¼•',  # ì¿ í° í• ì¸
            r'Qãƒã‚¤ãƒ³ãƒˆ',  # Qí¬ì¸íŠ¸ ê´€ë ¨
            r'è¿”å“.*æ¡ˆå†…',  # ë°˜í’ˆ ì•ˆë‚´
            r'é…é€.*æ¡ˆå†…',  # ë°°ì†¡ ì•ˆë‚´
            r'Qoo10',  # ì‚¬ì´íŠ¸ëª…
            r'ãƒ›ãƒ¼ãƒ ',  # í™ˆ
            r'Home',
            r'ãƒˆãƒƒãƒ—',  # íƒ‘
            r'Top',
            r'å•†å“å',  # ìƒí’ˆëª… (ë ˆì´ë¸”)
            r'å•†å“è©³ç´°',  # ìƒí’ˆ ìƒì„¸
        ]

        default_selectors = [
            'h1.product-name',
            'h1[itemprop="name"]',
            '.product_name',
            '#goods_name',
            '.goods_name',
            '[data-product-name]',  # data ì†ì„±ì—ì„œ ì¶”ì¶œ
            '.goods_title',  # Qoo10 íŠ¹ì • í´ë˜ìŠ¤
            'h1',  # h1 íƒœê·¸ (í•˜ì§€ë§Œ ì œì™¸ íŒ¨í„´ í™•ì¸)
            'title',  # fallbackìœ¼ë¡œ title íƒœê·¸ë„ í™•ì¸
        ]

        def extract_func(soup_obj: BeautifulSoup, selector: Optional[str]) -> str:
            # selectorê°€ ìˆëŠ” ê²½ìš° ìš°ì„  ì‹œë„
            if selector:
                if selector == 'title':
                    # title íƒœê·¸ì—ì„œ ìƒí’ˆëª… ì¶”ì¶œ (Qoo10 í˜•ì‹: "ìƒí’ˆëª… | Qoo10")
                    title_elem = soup_obj.find('title')
                    if title_elem:
                        title_text = title_elem.get_text(strip=True)
                        # "|" ë˜ëŠ” "ï½œ"ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²« ë²ˆì§¸ ë¶€ë¶„ ì¶”ì¶œ
                        if '|' in title_text:
                            name = title_text.split('|')[0].strip()
                        elif 'ï½œ' in title_text:
                            name = title_text.split('ï½œ')[0].strip()
                        else:
                            name = title_text

                        # "Qoo10" ì œê±°
                        name = name.replace('Qoo10', '').replace('[Qoo10]', '').strip()

                        # ì œì™¸ íŒ¨í„´ í™•ì¸
                        if name and len(name) > 3:
                            excluded = False
                            for pattern in exclude_patterns:
                                if re.search(pattern, name):
                                    excluded = True
                                    break
                            if not excluded:
                                return name

                elif selector == '[data-product-name]':
                    # data ì†ì„±ì—ì„œ ì¶”ì¶œ
                    elem = soup_obj.select_one(selector)
                    if elem:
                        name = elem.get('data-product-name') or elem.get_text(strip=True)
                        if name and len(name) > 3:
                            excluded = False
                            for pattern in exclude_patterns:
                                if re.search(pattern, name):
                                    excluded = True
                                    break
                            if not excluded:
                                return name

                else:
                    elem = soup_obj.select_one(selector)
                    if elem:
                        text = elem.get_text(strip=True)
                        # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì œì™¸)
                        if text and len(text) > 3:
                            excluded = False
                            for pattern in exclude_patterns:
                                if re.search(pattern, text):
                                    excluded = True
                                    break
                            if not excluded and text not in ['Qoo10', 'ãƒ›ãƒ¼ãƒ ', 'Home', 'ãƒˆãƒƒãƒ—', 'Top', 'å•†å“å']:
                                return text

            # selector ê¸°ë°˜ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°: í˜ì´ì§€ ë‚´ h1 íƒœê·¸ì—ì„œ ì¶”ì¶œ
            h1_tags = soup_obj.find_all('h1')
            for h1 in h1_tags:
                text = h1.get_text(strip=True)
                if text and len(text) > 10:
                    excluded = False
                    for pattern in exclude_patterns:
                        if re.search(pattern, text):
                            excluded = True
                            break
                    if not excluded and text not in ['Qoo10', 'ãƒ›ãƒ¼ãƒ ', 'Home', 'ãƒˆãƒƒãƒ—', 'Top', 'å•†å“å', 'å•†å“è©³ç´°']:
                        return text

            return "ìƒí’ˆëª… ì—†ìŒ"

        result = self._extract_with_learning(
            "product_name",
            soup,
            default_selectors,
            extract_func,
        )

        # ê²°ê³¼ ê²€ì¦ ë° ì •ì œ
        if result and result != "ìƒí’ˆëª… ì—†ìŒ":
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            result = ' '.join(result.split())
            # íŠ¹ìˆ˜ ë¬¸ì ì •ì œ: ëŒ€ê´„í˜¸ ì œê±°
            result = re.sub(r'^\[.*?\]\s*', '', result)  # ì•ìª½ ëŒ€ê´„í˜¸ ì œê±°
            result = re.sub(r'\s*\[.*?\]$', '', result)  # ë’¤ìª½ ëŒ€ê´„í˜¸ ì œê±°
            result = re.sub(r'\[.*?\]', '', result)  # ì¤‘ê°„ ëŒ€ê´„í˜¸ ì œê±°
            # íŠ¹ìˆ˜ ë¬¸ì ì •ì œ (í•„ìš”ì‹œ)
            result = result.strip()

        # #region agent log
        _log_debug("debug-session", "run1", "A", "crawler.py:_extract_product_name", "ìƒí’ˆëª… ì¶”ì¶œ ì™„ë£Œ", {
            "result": result[:100] if result else "",
            "is_empty": result == "ìƒí’ˆëª… ì—†ìŒ" or not result,
        })
        # #endregion

        return result
    
    def _extract_category(self, soup: BeautifulSoup) -> Optional[str]:
        """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (AI í•™ìŠµ ê¸°ë°˜) - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        default_selectors = [
            'meta[property="product:category"]',
            'nav.breadcrumb a',
            'ol.breadcrumb a',
            '.category',
            'nav[class*="breadcrumb"] a',
            'ol[class*="breadcrumb"] a',
            'a[href*="/category/"]',
            'a[href*="/cat/"]'
        ]
        
        def extract_func(soup_obj, selector):
            if selector:
                if selector.startswith('meta'):
                    elem = soup_obj.find('meta', {'property': 'product:category'})
                    if elem:
                        return elem.get('content')
                else:
                    elems = soup_obj.select(selector)
                    if elems:
                        # ë§ˆì§€ë§‰ ë§í¬ê°€ ë³´í†µ ì¹´í…Œê³ ë¦¬
                        for elem in reversed(elems):
                            text = elem.get_text(strip=True)
                            href = elem.get('href', '')
                            # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                            if '/category/' in href or '/cat/' in href:
                                category_match = re.search(r'/(?:category|cat)/([^/]+)', href)
                                if category_match:
                                    return category_match.group(1)
                            # í…ìŠ¤íŠ¸ê°€ ì˜ë¯¸ìˆëŠ” ê²½ìš° (ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­)
                            if text and len(text) > 2 and text not in ['ãƒ›ãƒ¼ãƒ ', 'Home', 'ãƒˆãƒƒãƒ—', 'Top']:
                                # ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­
                                translated_text = self._translate_jp_to_kr(text)
                                return translated_text
            
            # ê¸°ë³¸ ë°©ë²•: ë©”íƒ€ íƒœê·¸
            category_elem = soup_obj.find('meta', {'property': 'product:category'})
            if category_elem:
                return category_elem.get('content')
            
            # ë¸Œë ˆë“œí¬ëŸ¼ì—ì„œ ì¶”ì¶œ
            breadcrumb = soup_obj.find('nav', class_=re.compile(r'breadcrumb', re.I)) or \
                        soup_obj.find('ol', class_=re.compile(r'breadcrumb', re.I))
            if breadcrumb:
                links = breadcrumb.find_all('a')
                if len(links) > 1:
                    # ë§ˆì§€ë§‰ ë§í¬ê°€ ì¹´í…Œê³ ë¦¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                    last_link = links[-1]
                    text = last_link.get_text(strip=True)
                    if text and len(text) > 2:
                        return text
            
            # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„
            category_links = soup_obj.find_all('a', href=re.compile(r'/category/|/cat/'))
            if category_links:
                for link in category_links:
                    href = link.get('href', '')
                    match = re.search(r'/(?:category|cat)/([^/]+)', href)
                    if match:
                        return match.group(1)
            
            return None
        
        return self._extract_with_learning(
            "category",
            soup,
            default_selectors,
            extract_func
        )
    
    def _extract_brand(self, soup: BeautifulSoup) -> Optional[str]:
        """ë¸Œëœë“œ ì¶”ì¶œ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)"""
        brand_elem = soup.find('meta', {'property': 'product:brand'})
        if brand_elem:
            return brand_elem.get('content')
        
        # í˜ì´ì§€ì—ì„œ ë¸Œëœë“œ ì •ë³´ ì°¾ê¸° (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        brand_pattern = self._create_jp_kr_regex("ãƒ–ãƒ©ãƒ³ãƒ‰", "ë¸Œëœë“œ")
        brand_text = soup.find(string=re.compile(f'{brand_pattern}|Brand', re.I))
        if brand_text:
            parent = brand_text.find_parent()
            if parent:
                brand_value = parent.get_text(strip=True).split(':')[-1].strip()
                # ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­
                brand_value = self._translate_jp_to_kr(brand_value)
                return brand_value
        
        return None
    
    def _extract_price(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ (AI í•™ìŠµ ê¸°ë°˜) - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„  ë° ì •í™•ë„ í–¥ìƒ"""
        # #region agent log
        _log_debug("debug-session", "run1", "B", "crawler.py:_extract_price", "ê°€ê²© ì •ë³´ ì¶”ì¶œ ì‹œì‘", {})
        # #endregion
        
        price_data = {
            "original_price": None,
            "sale_price": None,
            "discount_rate": 0,
            "coupon_discount": None,  # ì¿ í° í• ì¸ ì •ë³´ ì¶”ê°€
            "qpoint_info": None  # Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ì¶”ê°€
        }
        
        # íŒë§¤ê°€ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„, ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        sale_price_selectors = [
            '.price',
            '.product-price',
            '[itemprop="price"]',
            '.sale_price',
            '#price',
            '.goods_price',
            'td.price',
            'span.price',
            'div.price',
            '[class*="price"]',  # priceê°€ í¬í•¨ëœ í´ë˜ìŠ¤
            '[data-price]'  # data ì†ì„±
        ]
        
        # "å•†å“ä¾¡æ ¼" ë˜ëŠ” "ìƒí’ˆê°€ê²©" í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œ ì°¾ê¸° (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        price_pattern = self._create_jp_kr_regex("å•†å“ä¾¡æ ¼", "ìƒí’ˆê°€ê²©")
        price_section = soup.find(string=re.compile(f'{price_pattern}[ï¼š:]|ê°€ê²©[ï¼š:]|ä¾¡æ ¼[ï¼š:]', re.I))
        if price_section:
            parent = price_section.find_parent()
            if parent:
                # ë¶€ëª¨ ìš”ì†Œì—ì„œ ê°€ê²© ìˆ«ì ì°¾ê¸°
                price_text = parent.get_text()
                # "å•†å“ä¾¡æ ¼: 4,562å††" ë˜ëŠ” "ìƒí’ˆê°€ê²©: 4,562å††" ê°™ì€ íŒ¨í„´ì—ì„œ ì¶”ì¶œ
                price_match = re.search(f'{price_pattern}[ï¼š:]\s*(\d{{1,3}}(?:,\d{{3}})*)å††', price_text)
                if price_match:
                    price = self._parse_price(price_match.group(1))
                    if price:
                        price_data["sale_price"] = price
                else:
                    # ì¼ë°˜ì ì¸ ìˆ«ì ì¶”ì¶œ
                    price = self._parse_price(price_text)
                    if price:
                        price_data["sale_price"] = price
        
        # ì„ íƒìë¥¼ í†µí•œ ê°€ê²© ì¶”ì¶œ
        for selector in sale_price_selectors:
            price_elem = soup.select_one(selector)
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                price = self._parse_price(price_text)
                if price and not price_data["sale_price"]:
                    price_data["sale_price"] = price
                    break
        
        # ì •ê°€ ì°¾ê¸° (ì·¨ì†Œì„ ì´ ìˆëŠ” ê°€ê²©) - ë” ì •í™•í•œ íŒ¨í„´ ë§¤ì¹­
        # "~~29,400å††~~" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
        price_text_all = soup.get_text()
        
        # íŒ¨í„´ 1: ~~ì •ê°€~~ í˜•ì‹
        strikethrough_pattern = re.search(r'~~(\d{1,3}(?:,\d{3})*)å††~~', price_text_all)
        if strikethrough_pattern:
            original_price = self._parse_price(strikethrough_pattern.group(1))
            if original_price:
                price_data["original_price"] = original_price
        
        # íŒ¨í„´ 2: HTML íƒœê·¸ì—ì„œ ì°¾ê¸°
        if not price_data["original_price"]:
            original_price_patterns = [
                soup.find(class_=re.compile(r'original|ì •ê°€|å®šä¾¡|å…ƒã®ä¾¡æ ¼|å…ƒä¾¡æ ¼', re.I)),
                soup.find('del'),
                soup.find('s'),
                soup.find(string=re.compile(r'~~\d+å††|å®šä¾¡.*\d+å††'))
            ]
            
            for pattern in original_price_patterns:
                if pattern:
                    if hasattr(pattern, 'get_text'):
                        original_text = pattern.get_text(strip=True)
                    else:
                        # string ê°ì²´ì¸ ê²½ìš°
                        parent = pattern.find_parent()
                        if parent:
                            original_text = parent.get_text(strip=True)
                        else:
                            original_text = str(pattern)
                    
                    original_price = self._parse_price(original_text)
                    if original_price and original_price > 0:
                        # íŒë§¤ê°€ë³´ë‹¤ ë†’ì€ì§€ í™•ì¸ (ì •ê°€ëŠ” ë³´í†µ íŒë§¤ê°€ë³´ë‹¤ ë†’ìŒ)
                        # None ì²´í¬ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ íƒ€ì… ì˜¤ë¥˜ ë°©ì§€
                        sale_price = price_data.get("sale_price")
                        if sale_price is None or original_price > sale_price:
                            price_data["original_price"] = original_price
                            break
        
        # ì¿ í° í• ì¸ ì •ë³´ ì¶”ì¶œ - ë” ì •í™•í•œ íŒ¨í„´ ë§¤ì¹­ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        coupon_pattern = self._create_jp_kr_regex("ã‚¯ãƒ¼ãƒãƒ³å‰²å¼•", "ì¿ í°í• ì¸")
        discount_pattern = self._create_jp_kr_regex("å‰²å¼•", "í• ì¸")
        coupon_section = soup.find(string=re.compile(f'{coupon_pattern}|{discount_pattern}', re.I))
        if coupon_section:
            parent = coupon_section.find_parent()
            if parent:
                coupon_text = parent.get_text()
                # "ãƒ—ãƒ©ã‚¹(\d+)å‰²å¼•" ë˜ëŠ” "í”ŒëŸ¬ìŠ¤(\d+)í• ì¸" ë˜ëŠ” "æœ€å¤§(\d+)å††" ë˜ëŠ” "ìµœëŒ€(\d+)ì—”" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
                plus_pattern = self._create_jp_kr_regex("ãƒ—ãƒ©ã‚¹", "í”ŒëŸ¬ìŠ¤")
                max_pattern = self._create_jp_kr_regex("æœ€å¤§", "ìµœëŒ€")
                coupon_match = re.search(f'{plus_pattern}(\d+){discount_pattern}|{max_pattern}(\d+)å††|(\d+)å††{discount_pattern}', coupon_text)
                if coupon_match:
                    discount = coupon_match.group(1) or coupon_match.group(2) or coupon_match.group(3)
                    if discount and discount.isdigit():
                        price_data["coupon_discount"] = int(discount)
        
        # ì¶”ê°€ ì‹œë„: "ã‚¯ãƒ¼ãƒãƒ³å‰²å¼•_(0)_" ë˜ëŠ” "ì¿ í°í• ì¸_(0)_" ê°™ì€ íŒ¨í„´ë„ í™•ì¸
        if not price_data["coupon_discount"]:
            coupon_text_all = soup.get_text()
            coupon_pattern = self._create_jp_kr_regex("ã‚¯ãƒ¼ãƒãƒ³å‰²å¼•", "ì¿ í°í• ì¸")
            coupon_match = re.search(f'{coupon_pattern}[_\s]*\((\d+)\)', coupon_text_all)
            if coupon_match:
                discount = coupon_match.group(1)
                if discount.isdigit():
                    price_data["coupon_discount"] = int(discount)
        
        # Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „, ìƒì„¸ ì •ë³´ëŠ” _extract_qpoint_infoì—ì„œ ì¶”ì¶œ)
        # ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›
        qpoint_pattern = self._create_jp_kr_regex("Qãƒã‚¤ãƒ³ãƒˆ", "Qí¬ì¸íŠ¸")
        qpoint_get_pattern = self._create_jp_kr_regex("Qãƒã‚¤ãƒ³ãƒˆç²å¾—", "Qí¬ì¸íŠ¸íšë“")
        qpoint_section = soup.find(string=re.compile(f'{qpoint_pattern}|{qpoint_get_pattern}'))
        if qpoint_section:
            parent = qpoint_section.find_parent()
            if parent:
                qpoint_text = parent.get_text()
                max_pattern = self._create_jp_kr_regex("æœ€å¤§", "ìµœëŒ€")
                qpoint_match = re.search(f'{max_pattern}(\d+)P', qpoint_text)
                if qpoint_match:
                    price_data["qpoint_info"] = int(qpoint_match.group(1))
        
        # í• ì¸ìœ¨ ê³„ì‚° (ì •í™•ë„ í–¥ìƒ)
        original_price = price_data.get("original_price")
        sale_price = price_data.get("sale_price")
        if original_price and sale_price and original_price > sale_price:
            discount = original_price - sale_price
            price_data["discount_rate"] = int((discount / original_price) * 100)
        elif original_price and sale_price:
            # ì •ê°€ê°€ íŒë§¤ê°€ë³´ë‹¤ ë‚®ìœ¼ë©´ ì˜ëª»ëœ ë°ì´í„°
            price_data["original_price"] = None
        
        # ê°€ê²© ìœ íš¨ì„± ê²€ì¦ ë° í•„í„°ë§ (ë¹„ì •ìƒì ì¸ ê°’ ì œê±°)
        sale_price = price_data.get("sale_price")
        if sale_price is not None:
            # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ í™•ì¸ (100ì—” ~ 1,000,000ì—”)
            if not (100 <= sale_price <= 1000000):
                # ë¹„ì •ìƒì ì¸ ê°’ì´ë©´ nullë¡œ ì„¤ì •
                price_data["sale_price"] = None
        
        original_price = price_data.get("original_price")
        if original_price is not None:
            # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ í™•ì¸ (100ì—” ~ 1,000,000ì—”)
            if not (100 <= original_price <= 1000000):
                # ë¹„ì •ìƒì ì¸ ê°’ì´ë©´ nullë¡œ ì„¤ì •
                price_data["original_price"] = None
            else:
                # ì •ê°€ê°€ íŒë§¤ê°€ë³´ë‹¤ ë‚®ìœ¼ë©´ ì˜ëª»ëœ ë°ì´í„°
                sale_price = price_data.get("sale_price")
                if sale_price is not None and original_price < sale_price:
                    price_data["original_price"] = None
        
        # ë°ì´í„° ê²€ì¦: íŒë§¤ê°€ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜
        if not price_data["sale_price"]:
            # ìµœí›„ì˜ ì‹œë„: í˜ì´ì§€ ì „ì²´ì—ì„œ ê°€ê²© íŒ¨í„´ ì°¾ê¸°
            all_text = soup.get_text()
            price_patterns = [
                r'å•†å“ä¾¡æ ¼[ï¼š:]\s*(\d{1,3}(?:,\d{3})*)å††',  # ìƒí’ˆê°€ê²© íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                r'ä¾¡æ ¼[ï¼š:]\s*(\d{1,3}(?:,\d{3})*)å††',
                r'(\d{1,3}(?:,\d{3})*)å††',  # ì¼ë°˜ì ì¸ ê°€ê²© íŒ¨í„´
                r'Â¥\s*(\d{1,3}(?:,\d{3})*)'
            ]
            for pattern in price_patterns:
                matches = re.findall(pattern, all_text)
                if matches:
                    prices = [self._parse_price(m) for m in matches if self._parse_price(m)]
                    if prices:
                        # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„ (100ì—” ~ 1,000,000ì—”) - ì—¬ëŸ¬ ê°€ê²© ì¤‘ ìµœì†Œê°’ ì„ íƒ
                        valid_prices = [p for p in prices if 100 <= p <= 1000000]
                        if valid_prices:
                            # ì—¬ëŸ¬ ê°€ê²©ì´ ìˆìœ¼ë©´ ìµœì†Œê°’ì„ íŒë§¤ê°€ë¡œ ì¶”ì • (ì¼ë°˜ì ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ê°€ê²©)
                            price_data["sale_price"] = min(valid_prices)
                            break
        
        # #region agent log
        _log_debug("debug-session", "run1", "B", "crawler.py:_extract_price", "ê°€ê²© ì •ë³´ ì¶”ì¶œ ì™„ë£Œ", {
            "sale_price": price_data.get("sale_price"),
            "original_price": price_data.get("original_price"),
            "discount_rate": price_data.get("discount_rate"),
            "has_coupon": bool(price_data.get("coupon_discount")),
            "is_empty": not price_data.get("sale_price")
        })
        # #endregion
        
        return price_data
    
    def _parse_price(self, price_text: Optional[str]) -> Optional[int]:
        """ê°€ê²© í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜"""
        if not price_text:
            return None
        # ìˆ«ìì™€ ì‰¼í‘œë§Œ ì¶”ì¶œ
        numbers = re.sub(r'[^\d,]', '', str(price_text).replace(',', ''))
        try:
            return int(numbers) if numbers else None
        except:
            return None
    
    def _extract_images(self, soup: BeautifulSoup) -> Dict[str, List[str]]:
        """ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„  (itemGoods ì˜ì—­ í¬í•¨)"""
        images = {
            "thumbnail": None,
            "detail_images": [],
            "item_goods_images": []  # itemGoods ì˜ì—­ì˜ ëª¨ë“  ì´ë¯¸ì§€
        }
        
        # ì¸ë„¤ì¼ ì´ë¯¸ì§€ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        thumbnail_selectors = [
            'img.product-thumbnail',
            'img[itemprop="image"]',
            '.product-image img',
            'img.main-image',
            '#goods_img img',
            '.goods_img img',
            '.thumbnail img',
            'img[class*="thumbnail"]',
            'img[class*="main"]',
            'img[class*="product"]'
        ]
        
        for selector in thumbnail_selectors:
            img = soup.select_one(selector)
            if img:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = 'https://www.qoo10.jp' + src
                    images["thumbnail"] = src
                    break
        
        # itemGoods ì˜ì—­ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ ì´ë¯¸ì§€ ì˜ì—­)
        item_goods_div = soup.find('div', {'id': 'itemGoods'})
        if item_goods_div:
            seen_item_images = set()
            # itemGoods ë‚´ì˜ ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
            item_imgs = item_goods_div.find_all('img')
            for img in item_imgs:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = 'https://www.qoo10.jp' + src
                    
                    # ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸
                    if src not in seen_item_images and ('http' in src or src.startswith('//')):
                        # ì‘ì€ ì•„ì´ì½˜ì´ë‚˜ ë°°ë„ˆ ì´ë¯¸ì§€ ì œì™¸
                        if not any(exclude in src.lower() for exclude in ['icon', 'logo', 'banner', 'button']):
                            images["item_goods_images"].append(src)
                            seen_item_images.add(src)
        
        # ìƒì„¸ ì´ë¯¸ì§€ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        detail_img_selectors = [
            '.product-detail img',
            '.detail-images img',
            '.product-images img',
            '#goods_detail img',
            '.goods_detail img',
            'div[class*="detail"] img',
            'div[class*="description"] img',
            'div[class*="content"] img'
        ]
        
        seen_images = set()
        if images["thumbnail"]:
            seen_images.add(images["thumbnail"])
        # item_goods_imagesë„ ì¤‘ë³µ ì²´í¬ì— ì¶”ê°€
        for img_url in images["item_goods_images"]:
            seen_images.add(img_url)
        
        for selector in detail_img_selectors:
            imgs = soup.select(selector)
            for img in imgs:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = 'https://www.qoo10.jp' + src
                    
                    # ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸
                    if src not in seen_images and ('http' in src or src.startswith('//')):
                        # ì‘ì€ ì•„ì´ì½˜ì´ë‚˜ ë°°ë„ˆ ì´ë¯¸ì§€ ì œì™¸
                        if not any(exclude in src.lower() for exclude in ['icon', 'logo', 'banner', 'button']):
                            images["detail_images"].append(src)
                            seen_images.add(src)
        
        # #region agent log
        _log_debug("debug-session", "run1", "E", "crawler.py:_extract_images", "ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ", {
            "has_thumbnail": bool(images.get("thumbnail")),
            "detail_images_count": len(images.get("detail_images", [])),
            "item_goods_images_count": len(images.get("item_goods_images", [])),
            "total_images": len(images.get("detail_images", [])) + len(images.get("item_goods_images", [])) + (1 if images.get("thumbnail") else 0),
            "is_empty": not images.get("thumbnail") and len(images.get("detail_images", [])) == 0 and len(images.get("item_goods_images", [])) == 0
        })
        # #endregion
        
        return images
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """ìƒí’ˆ ì„¤ëª… ì¶”ì¶œ (AI í•™ìŠµ ê¸°ë°˜) - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„  ë° ì •í™•ë„ í–¥ìƒ"""
        default_selectors = [
            '.product-description',
            '[itemprop="description"]',
            '.description',
            '.product-detail',
            '#goods_detail',
            '.goods_detail',
            '.detail_description',
            'div[class*="detail"]',
            'div[class*="description"]',
            '[id*="detail"]',  # idì— detailì´ í¬í•¨ëœ ìš”ì†Œ
            '[id*="description"]'  # idì— descriptionì´ í¬í•¨ëœ ìš”ì†Œ
        ]
        
        def extract_func(soup_obj, selector):
            if selector:
                desc_elem = soup_obj.select_one(selector)
                if desc_elem:
                    # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    text = desc_elem.get_text(separator='\n', strip=True)
                    # ì˜ë¯¸ìˆëŠ” ì„¤ëª…ì¸ì§€ í™•ì¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì œì™¸)
                    if text and len(text) > 50:
                        # "ìƒí’ˆ ì„¤ëª…", "å•†å“èª¬æ˜" ê°™ì€ ì œëª© ì œê±°
                        text = re.sub(r'^(å•†å“èª¬æ˜|ìƒí’ˆ\s*ì„¤ëª…|Description)[ï¼š:]\s*', '', text, flags=re.I)
                        if len(text.strip()) > 50:
                            return text.strip()
            
            # ì¶”ê°€ ì‹œë„: ë©”íƒ€ description íƒœê·¸ í™•ì¸
            meta_desc = soup_obj.find('meta', {'name': 'description'})
            if meta_desc:
                desc_content = meta_desc.get('content', '')
                if desc_content and len(desc_content) > 50:
                    return desc_content
            
            # JSON-LD ìŠ¤í‚¤ë§ˆì—ì„œ ì„¤ëª… ì¶”ì¶œ
            json_ld_scripts = soup_obj.find_all('script', {'type': 'application/ld+json'})
            for script in json_ld_scripts:
                try:
                    import json
                    data = json.loads(script.string)
                    # ì¤‘ì²©ëœ êµ¬ì¡° ì²˜ë¦¬
                    if isinstance(data, dict):
                        if 'description' in data:
                            desc = data['description']
                            if isinstance(desc, str) and len(desc) > 50:
                                return desc
                        # @graph êµ¬ì¡° ì²˜ë¦¬
                        if '@graph' in data:
                            for item in data['@graph']:
                                if isinstance(item, dict) and 'description' in item:
                                    desc = item['description']
                                    if isinstance(desc, str) and len(desc) > 50:
                                        return desc
                except (json.JSONDecodeError, TypeError, AttributeError):
                    continue
            
            # ìµœí›„ì˜ ì‹œë„: í˜ì´ì§€ì—ì„œ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
            # ìƒí’ˆëª… ë‹¤ìŒì— ì˜¤ëŠ” ê¸´ í…ìŠ¤íŠ¸ê°€ ì„¤ëª…ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            all_divs = soup_obj.find_all('div')
            for div in all_divs:
                text = div.get_text(separator=' ', strip=True)
                # 100ì ì´ìƒì´ê³ , ìƒí’ˆëª…ì´ë‚˜ ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
                if len(text) >= 100:
                    # ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì œì™¸
                    if not any(exclude in text[:50] for exclude in ['ãƒ›ãƒ¼ãƒ ', 'Home', 'ãƒˆãƒƒãƒ—', 'Top', 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'Menu']):
                        return text[:2000]  # ìµœëŒ€ 2000ìê¹Œì§€ë§Œ
            
            return ""
        
        result = self._extract_with_learning(
            "description",
            soup,
            default_selectors,
            extract_func
        )
        
        # ê²°ê³¼ ì •ì œ
        if result:
            # HTML íƒœê·¸ ì œê±° (í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” ê²½ìš°)
            result = re.sub(r'<[^>]+>', '', result)
            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            result = ' '.join(result.split())
            # ìµœëŒ€ ê¸¸ì´ ì œí•œ
            if len(result) > 5000:
                result = result[:5000] + "..."
        
        # #region agent log
        _log_debug("debug-session", "run1", "D", "crawler.py:_extract_description", "ìƒí’ˆ ì„¤ëª… ì¶”ì¶œ ì™„ë£Œ", {
            "description_length": len(result) if result else 0,
            "is_empty": not result or len(result) < 50
        })
        # #endregion
        
        return result
    
    def _extract_search_keywords(self, soup: BeautifulSoup) -> List[str]:
        """ê²€ìƒ‰ì–´ ì¶”ì¶œ"""
        keywords = []
        
        # ë©”íƒ€ í‚¤ì›Œë“œ
        meta_keywords = soup.find('meta', {'name': 'keywords'})
        if meta_keywords:
            keywords.extend(meta_keywords.get('content', '').split(','))
        
        # ê²€ìƒ‰ì–´ í•„ë“œ (í˜ì´ì§€ ë‚´)
        search_keyword_elem = soup.find('input', {'name': 'search_keyword'})
        if search_keyword_elem:
            keywords.append(search_keyword_elem.get('value', ''))
        
        return [k.strip() for k in keywords if k.strip()]
    
    def _extract_reviews(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ë¦¬ë·° ì •ë³´ ì¶”ì¶œ - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„  ë° ì •í™•ë„ í–¥ìƒ"""
        # #region agent log
        _log_debug("debug-session", "run1", "C", "crawler.py:_extract_reviews", "ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì‹œì‘", {})
        # #endregion
        
        reviews_data = {
            "rating": 0.0,
            "review_count": 0,
            "reviews": []
        }
        
        # í‰ì  ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„, ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        rating_patterns = [
            ('meta', {'itemprop': 'ratingValue'}),
            ('meta', {'property': 'product:ratingValue'}),
            ('span', {'class': re.compile(r'rating|star|score', re.I)}),
            ('div', {'class': re.compile(r'rating|star|score', re.I)}),
        ]
        
        for tag, attrs in rating_patterns:
            rating_elem = soup.find(tag, attrs)
            if rating_elem:
                rating_text = rating_elem.get('content') or rating_elem.get_text(strip=True)
                try:
                    # "4.8(150)" ê°™ì€ í˜•ì‹ì—ì„œ í‰ì  ì¶”ì¶œ
                    rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                    if rating_match:
                        rating_value = float(rating_match.group(1))
                        # í•©ë¦¬ì ì¸ ë²”ìœ„ í™•ì¸ (0.0 ~ 5.0)
                        if 0.0 <= rating_value <= 5.0:
                            reviews_data["rating"] = rating_value
                            break
                except (ValueError, AttributeError):
                    pass
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‰ì  íŒ¨í„´ ì°¾ê¸° (ì˜ˆ: "4.6(184)")
        if reviews_data["rating"] == 0.0:
            rating_text_pattern = re.search(r'(\d+\.?\d*)\s*\((\d+)\)', soup.get_text())
            if rating_text_pattern:
                try:
                    rating_value = float(rating_text_pattern.group(1))
                    if 0.0 <= rating_value <= 5.0:
                        reviews_data["rating"] = rating_value
                        # ë¦¬ë·° ìˆ˜ë„ í•¨ê»˜ ì¶”ì¶œ
                        review_count = int(rating_text_pattern.group(2))
                        reviews_data["review_count"] = review_count
                except (ValueError, AttributeError):
                    pass
        
        # ë¦¬ë·° ìˆ˜ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„, ì •í™•ë„ í–¥ìƒ) - ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›
        if reviews_data["review_count"] == 0:
            review_pattern = self._create_jp_kr_regex("ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ë¦¬ë·°")
            review_count_patterns = [
                ('meta', {'itemprop': 'reviewCount'}),
                ('meta', {'property': 'product:reviewCount'}),
                (None, {'string': re.compile(f'{review_pattern}.*\\((\\d+)\\)|review.*\\((\\d+)\\)', re.I)}),
            ]
            
            for tag, attrs in review_count_patterns:
                if tag:
                    review_count_elem = soup.find(tag, attrs)
                else:
                    # string ê²€ìƒ‰
                    review_count_elem = soup.find(string=attrs.get('string'))
                
                if review_count_elem:
                    if hasattr(review_count_elem, 'get'):
                        count_text = review_count_elem.get('content', '')
                    else:
                        # string ê°ì²´ì¸ ê²½ìš°
                        count_text = str(review_count_elem)
                        # "4.8(150)" ê°™ì€ í˜•ì‹ì—ì„œ ë¦¬ë·° ìˆ˜ ì¶”ì¶œ
                        count_match = re.search(r'\((\d+)\)', count_text)
                        if count_match:
                            count_value = int(count_match.group(1))
                            if count_value > 0:
                                reviews_data["review_count"] = count_value
                                break
                    
                    # ìˆ«ì ì¶”ì¶œ
                    numbers = re.findall(r'\d+', count_text)
                    if numbers:
                        count_value = int(numbers[0])
                        if count_value > 0:
                            reviews_data["review_count"] = count_value
                            break
        
        # CustomerReview ì˜ì—­ì˜ ìƒì„¸ ë¦¬ë·° ì¶”ì¶œ
        customer_review_div = soup.find('div', {'id': 'CustomerReview'}) or soup.find('div', {'class': 'sec_review'})
        if customer_review_div:
            # review_list ë‚´ì˜ ê° ë¦¬ë·° í•­ëª© ì¶”ì¶œ
            review_list = customer_review_div.find('ul', {'class': 'review_list'}) or customer_review_div.find('ul', {'id': 'reviews_wrapper'})
            if review_list:
                review_items = review_list.find_all('li', recursive=False)
                for review_item in review_items[:50]:  # ìµœëŒ€ 50ê°œ ë¦¬ë·°
                    review_detail = {}
                    
                    # í‰ì  ì¶”ì¶œ
                    review_star_area = review_item.find('div', {'class': 'review_star_area'})
                    if review_star_area:
                        score_elem = review_star_area.find('span', {'class': 'score'})
                        total_elem = review_star_area.find('span', {'class': 'total'})
                        if score_elem and total_elem:
                            try:
                                review_detail["rating"] = float(score_elem.get_text(strip=True))
                                review_detail["max_rating"] = float(total_elem.get_text(strip=True))
                            except (ValueError, AttributeError):
                                pass
                    
                    # ì‚¬ìš©ì ì •ë³´ ì¶”ì¶œ
                    review_user_info = review_item.find('div', {'class': 'review_user_info'})
                    if review_user_info:
                        spans = review_user_info.find_all('span')
                        if len(spans) > 0:
                            review_detail["user_name"] = spans[0].get_text(strip=True)
                        if len(spans) > 1:
                            review_detail["date"] = spans[1].get_text(strip=True)
                        # ì˜µì…˜ ì •ë³´ ì¶”ì¶œ
                        option_texts = []
                        for span in spans[2:]:
                            text = span.get_text(strip=True)
                            if text and text not in option_texts:
                                option_texts.append(text)
                        if option_texts:
                            review_detail["options"] = option_texts
                    
                    # ì˜µì…˜ ìƒì„¸ ì •ë³´
                    review_user_type = review_item.find('div', {'class': 'review_user_type'})
                    if review_user_type:
                        option_span = review_user_type.find('span')
                        if option_span:
                            review_detail["option_detail"] = option_span.get_text(strip=True)
                    
                    # ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    review_txt = review_item.find('p', {'class': 'review_txt'})
                    if review_txt:
                        review_detail["text"] = review_txt.get_text(strip=True)
                    
                    # ë¦¬ë·° ì´ë¯¸ì§€ ì¶”ì¶œ
                    review_photo = review_item.find('ul', {'class': 'review_photo'})
                    if review_photo:
                        review_images = []
                        for img_li in review_photo.find_all('li'):
                            img = img_li.find('img')
                            if img:
                                img_src = img.get('src') or img.get('data-src')
                                if img_src:
                                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                    if img_src.startswith('//'):
                                        img_src = 'https:' + img_src
                                    elif img_src.startswith('/'):
                                        img_src = 'https://www.qoo10.jp' + img_src
                                    review_images.append(img_src)
                        if review_images:
                            review_detail["images"] = review_images
                    
                    # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
                    review_button = review_item.find('div', {'class': 'review_button'})
                    if review_button:
                        count_elem = review_button.find('span', {'class': 'count'})
                        if count_elem:
                            try:
                                review_detail["like_count"] = int(count_elem.get_text(strip=True))
                            except (ValueError, AttributeError):
                                pass
                    
                    # ë¦¬ë·°ê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                    if review_detail.get("text") or review_detail.get("rating"):
                        reviews_data["reviews"].append(review_detail)
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œë„ ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ (fallback)
        if len(reviews_data["reviews"]) == 0:
            review_selectors = [
                '.review-item',
                '.review-text',
                '[itemprop="reviewBody"]',
                '.review_content',
                '.review-body',
                'div[class*="review"]',
                'p[class*="review"]'
            ]
            
            seen_reviews = set()
            for selector in review_selectors:
                review_elements = soup.select(selector)
                for elem in review_elements[:20]:  # ìµœëŒ€ 20ê°œ
                    review_text = elem.get_text(strip=True)
                    if review_text and len(review_text) > 10:  # ì˜ë¯¸ìˆëŠ” ë¦¬ë·°ì¸ì§€ í™•ì¸
                        # ì¤‘ë³µ ì œê±°
                        if review_text not in seen_reviews:
                            reviews_data["reviews"].append({"text": review_text})
                            seen_reviews.add(review_text)
                            if len(reviews_data["reviews"]) >= 10:  # ìµœëŒ€ 10ê°œ
                                break
                if len(reviews_data["reviews"]) >= 10:
                    break
        
        # review_countê°€ 0ì´ì§€ë§Œ reviews ë°°ì—´ì— ë¦¬ë·°ê°€ ìˆìœ¼ë©´ fallback
        if reviews_data["review_count"] == 0 and len(reviews_data["reviews"]) > 0:
            reviews_data["review_count"] = len(reviews_data["reviews"])
        
        # #region agent log
        _log_debug("debug-session", "run1", "C", "crawler.py:_extract_reviews", "ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ì™„ë£Œ", {
            "rating": reviews_data.get("rating"),
            "review_count": reviews_data.get("review_count"),
            "reviews_count": len(reviews_data.get("reviews", [])),
            "is_empty": not reviews_data.get("rating") and not reviews_data.get("review_count")
        })
        # #endregion
        
        return reviews_data
    
    def _extract_qna(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Q&A ì •ë³´ ì¶”ì¶œ - Question_Answer ì˜ì—­ì˜ ìƒì„¸ Q&A ì •ë³´ ì¶”ì¶œ"""
        qna_data = {
            "total_count": 0,
            "qna_list": []
        }
        
        # Question_Answer ì˜ì—­ ì°¾ê¸°
        qna_div = soup.find('div', {'id': 'Question_Answer'})
        if not qna_div:
            return qna_data
        
        # Q&A ì´ ê°œìˆ˜ ì¶”ì¶œ
        qna_count_elem = qna_div.find('span', {'id': 'qnaList_count_1'})
        if qna_count_elem:
            try:
                qna_data["total_count"] = int(qna_count_elem.get_text(strip=True))
            except (ValueError, AttributeError):
                pass
        
        # Q&A ëª©ë¡ ì¶”ì¶œ
        qna_board = qna_div.find('div', {'class': 'qna_board'})
        if qna_board:
            qna_list_div = qna_board.find('div', {'id': 'dv_lst'})
            if qna_list_div:
                # ê° Q&A í•­ëª© ì¶”ì¶œ
                qna_rows = qna_list_div.find_all('div', {'class': 'row'}, recursive=False)
                for row in qna_rows[:50]:  # ìµœëŒ€ 50ê°œ Q&A
                    qna_item = {}
                    
                    # ìƒíƒœ ì¶”ì¶œ (å›ç­”å®Œäº†, æœªå›ç­”, å‡¦ç†ä¸­ ë“±)
                    col1 = row.find('div', {'class': 'col1'})
                    if col1:
                        status_tag = col1.find('span', {'class': re.compile(r'tag')})
                        if status_tag:
                            qna_item["status"] = status_tag.get_text(strip=True)
                    
                    # ì§ˆë¬¸ ì œëª© ì¶”ì¶œ
                    col2 = row.find('div', {'class': 'col2'})
                    if col2:
                        title_link = col2.find('a')
                        if title_link:
                            qna_item["question_title"] = title_link.get_text(strip=True)
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    col3 = row.find('div', {'class': 'col3'})
                    if col3:
                        qna_item["date"] = col3.get_text(strip=True)
                    
                    # ì‘ì„±ì ì¶”ì¶œ
                    col4 = row.find('div', {'class': 'col4'})
                    if col4:
                        qna_item["author"] = col4.get_text(strip=True)
                    
                    # ì§ˆë¬¸ ë‚´ìš© ì¶”ì¶œ
                    mode_user = row.find('div', {'class': 'mode_user'})
                    if mode_user:
                        user_col2 = mode_user.find('div', {'class': 'col2'})
                        if user_col2:
                            cont = user_col2.find('div', {'class': 'cont'})
                            if cont:
                                qna_item["question"] = cont.get_text(strip=True)
                    
                    # ë‹µë³€ ë‚´ìš© ì¶”ì¶œ
                    mode_sllr = row.find('div', {'class': 'mode_sllr'})
                    if mode_sllr:
                        seller_col2 = mode_sllr.find('div', {'class': 'col2'})
                        if seller_col2:
                            cont = seller_col2.find('div', {'class': 'cont'})
                            if cont:
                                qna_item["answer"] = cont.get_text(strip=True)
                    
                    # Q&Aê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                    if qna_item.get("question") or qna_item.get("question_title"):
                        qna_data["qna_list"].append(qna_item)
        
        # total_countê°€ 0ì´ì§€ë§Œ qna_listì— Q&Aê°€ ìˆìœ¼ë©´ fallback
        if qna_data["total_count"] == 0 and len(qna_data["qna_list"]) > 0:
            qna_data["total_count"] = len(qna_data["qna_list"])
        
        # #region agent log
        _log_debug("debug-session", "run1", "C", "crawler.py:_extract_qna", "Q&A ì •ë³´ ì¶”ì¶œ ì™„ë£Œ", {
            "total_count": qna_data.get("total_count"),
            "qna_list_count": len(qna_data.get("qna_list", [])),
            "is_empty": qna_data.get("total_count") == 0 and len(qna_data.get("qna_list", [])) == 0
        })
        # #endregion
        
        return qna_data
    
    def _extract_goods_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ìƒí’ˆ ì •ë³´ ì˜ì—­ ì¶”ì¶œ - goods_info div ë‚´ì˜ ëª¨ë“  ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        goods_info = {
            "product_images": [],
            "brand": None,
            "product_title": None,
            "promotion_text": None,
            "price_info": {},
            "benefits": {},
            "shipping_info": {},
            "return_info": {},
            "options": [],
            "custom_name_field": None
        }
        
        # goods_info ì˜ì—­ ì°¾ê¸°
        goods_info_div = soup.find('div', {'class': 'goods_info'})
        if not goods_info_div:
            return goods_info
        
        # ìƒí’ˆ ì´ë¯¸ì§€ ì˜ì—­ (goods_img)
        goods_img_div = goods_info_div.find('div', {'class': 'goods_img'})
        if goods_img_div:
            # div_Default_Image ë‚´ì˜ ì´ë¯¸ì§€ ëª©ë¡ ì¶”ì¶œ
            default_image_div = goods_img_div.find('div', {'id': 'div_Default_Image'})
            if default_image_div:
                # ê¸°ë³¸ ì´ë¯¸ì§€
                basic_image_input = default_image_div.find('input', {'id': 'basic_image'})
                if basic_image_input:
                    basic_image_url = basic_image_input.get('value', '')
                    if basic_image_url:
                        goods_info["product_images"].append(basic_image_url)
                
                # ì¸ë„¤ì¼ ëª©ë¡ (ulIndicate)
                thumb_list = default_image_div.find('ul', {'id': 'ulIndicate'})
                if thumb_list:
                    thumb_items = thumb_list.find_all('li', {'name': 'liIndicateID'})
                    for item in thumb_items:
                        img = item.find('img')
                        if img:
                            img_src = img.get('src') or img.get('data-src')
                            if img_src:
                                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                if img_src.startswith('//'):
                                    img_src = 'https:' + img_src
                                elif img_src.startswith('/'):
                                    img_src = 'https://www.qoo10.jp' + img_src
                                if img_src not in goods_info["product_images"]:
                                    goods_info["product_images"].append(img_src)
                
                # í™•ëŒ€ ì´ë¯¸ì§€ ë ˆì´ì–´ (gei_layer_enlarge) ë‚´ì˜ ì´ë¯¸ì§€ ëª©ë¡
                enlarge_layer = goods_img_div.find('div', {'id': 'gei_layer_enlarge'})
                if enlarge_layer:
                    thumb_list_enlarge = enlarge_layer.find('ul', {'id': re.compile(r'gei_goodsimg')})
                    if thumb_list_enlarge:
                        enlarge_items = thumb_list_enlarge.find_all('li')
                        for item in enlarge_items:
                            large_img = item.get('large_img')
                            if large_img:
                                if large_img.startswith('//'):
                                    large_img = 'https:' + large_img
                                elif large_img.startswith('/'):
                                    large_img = 'https://www.qoo10.jp' + large_img
                                if large_img not in goods_info["product_images"]:
                                    goods_info["product_images"].append(large_img)
        
        # ìƒí’ˆ ìƒì„¸ ì •ë³´ ì˜ì—­ (goods_detail)
        goods_detail_div = goods_info_div.find('div', {'class': 'goods_detail'})
        if goods_detail_div:
            # ë¸Œëœë“œ ë° ìƒí’ˆëª…
            detail_title = goods_detail_div.find('div', {'class': 'detail_title'})
            if detail_title:
                brand_link = detail_title.find('a', {'class': 'title_brand'})
                if brand_link:
                    goods_info["brand"] = brand_link.get_text(strip=True)
                
                text_title = detail_title.find('div', {'class': 'text_title'})
                if text_title:
                    # ë¸Œëœë“œ ë§í¬ë¥¼ ì œì™¸í•œ í…ìŠ¤íŠ¸ê°€ ìƒí’ˆëª…
                    brand_link_clone = text_title.find('a', {'class': 'title_brand'})
                    if brand_link_clone:
                        brand_link_clone.decompose()
                    goods_info["product_title"] = text_title.get_text(strip=True)
                
                promotion_p = detail_title.find('p', {'class': 'text_promotion'})
                if promotion_p:
                    goods_info["promotion_text"] = promotion_p.get_text(strip=True)
            
            # ê°€ê²© ì •ë³´
            info_area = goods_detail_div.find('ul', {'class': 'infoArea'})
            if info_area:
                # íŒë§¤ ê°€ê²©
                sell_price_dl = info_area.find('dl', {'id': 'dl_sell_price'})
                if sell_price_dl:
                    price_dd = sell_price_dl.find('dd')
                    if price_dd:
                        price_strong = price_dd.find('strong')
                        if price_strong:
                            price_text = price_strong.get_text(strip=True)
                            # ìˆ«ì ì¶”ì¶œ
                            price_match = re.search(r'([\d,]+)', price_text.replace(',', ''))
                            if price_match:
                                try:
                                    goods_info["price_info"]["selling_price"] = int(price_match.group(1).replace(',', ''))
                                except ValueError:
                                    pass
                
                # íƒ€ì„ì„¸ì¼ ê°€ê²©
                discount_info = info_area.find('span', {'id': re.compile(r'discount_info')})
                if discount_info:
                    dcprice_dl = discount_info.find('dl', {'class': 'q_dcprice'})
                    if dcprice_dl:
                        dcprice_dd = dcprice_dl.find('dd')
                        if dcprice_dd:
                            dcprice_strong = dcprice_dd.find('strong')
                            if dcprice_strong:
                                dcprice_text = dcprice_strong.get_text(strip=True)
                                dcprice_match = re.search(r'([\d,]+)', dcprice_text.replace(',', ''))
                                if dcprice_match:
                                    try:
                                        goods_info["price_info"]["timesale_price"] = int(dcprice_match.group(1).replace(',', ''))
                                    except ValueError:
                                        pass
                    
                    # ì„¸ì¼ ì‹œê°„ ì •ë³´
                    dsc_txt_dl = discount_info.find('dl', {'class': 'dsc_txt'})
                    if dsc_txt_dl:
                        dsc_dd = dsc_txt_dl.find('dd')
                        if dsc_dd:
                            sale_time_p = dsc_dd.find('p', {'class': 'fs_11'})
                            if sale_time_p:
                                goods_info["price_info"]["sale_time"] = sale_time_p.get_text(strip=True)
                
                # ì¶”ê°€ í˜œíƒ (Qí¬ì¸íŠ¸)
                benefit_li = info_area.find('li', {'id': re.compile(r'li_BenefitInfo')})
                if benefit_li:
                    super_point = benefit_li.find('span', {'class': 'super_point'})
                    if super_point:
                        point_strong = super_point.find('strong')
                        if point_strong:
                            goods_info["benefits"]["qpoint"] = point_strong.get_text(strip=True)
                
                # ë°°ì†¡ ì •ë³´ - infoData í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  li ì¤‘ì—ì„œ ë°°ì†¡ ê´€ë ¨ ì°¾ê¸°
                all_info_data = info_area.find_all('li', {'class': 'infoData'})
                shipping_li = None
                for li in all_info_data:
                    text = li.get_text()
                    if 'ë°œì†¡êµ­' in text or 'é€æ–™' in text or 'é…é€' in text or 'ë°°ì†¡' in text:
                        shipping_li = li
                        break
                
                if shipping_li:
                    # ë°œì†¡êµ­
                    shipping_dl = shipping_li.find('dl', {'name': 'shipping_panel_area'})
                    if shipping_dl:
                        shipping_dt = shipping_dl.find('dt')
                        shipping_dd = shipping_dl.find('dd')
                        if shipping_dt and shipping_dd:
                            key = shipping_dt.get_text(strip=True)
                            value = shipping_dd.get_text(strip=True)
                            goods_info["shipping_info"][key] = value
                    
                    # ë°°ì†¡ë¹„
                    shipping_fee_dl = shipping_li.find('dl', {'class': 'detailsArea'})
                    if shipping_fee_dl:
                        shipping_fee_dt = shipping_fee_dl.find('dt')
                        shipping_fee_dd = shipping_fee_dl.find('dd')
                        if shipping_fee_dt and shipping_fee_dd:
                            fee_text = shipping_fee_dd.get_text(strip=True)
                            if 'ë¬´ë£Œ' in fee_text or 'ç„¡æ–™' in fee_text or 'FREE' in fee_text.upper():
                                goods_info["shipping_info"]["shipping_fee"] = 0
                                goods_info["shipping_info"]["free_shipping"] = True
                            else:
                                fee_match = re.search(r'(\d+)', fee_text)
                                if fee_match:
                                    try:
                                        goods_info["shipping_info"]["shipping_fee"] = int(fee_match.group(1))
                                    except ValueError:
                                        pass
                    
                    # ë°°ì†¡ì¼
                    delivery_date_panel = shipping_li.find('div', {'id': re.compile(r'availabledatePanel')})
                    if delivery_date_panel:
                        delivery_dl = delivery_date_panel.find('dl', {'class': 'detailsArea'})
                        if delivery_dl:
                            delivery_dd = delivery_dl.find('dd')
                            if delivery_dd:
                                goods_info["shipping_info"]["delivery_date"] = delivery_dd.get_text(strip=True)
                
                # ë°˜í’ˆ ì •ë³´
                return_panel = info_area.find('div', {'id': re.compile(r'deliveryRuleExplain')})
                if return_panel:
                    return_li = return_panel.find('li', {'class': 'infoData'})
                    if return_li:
                        return_dl = return_li.find('dl', {'class': 'detailsArea'})
                        if return_dl:
                            return_dt = return_dl.find('dt')
                            return_dd = return_dl.find('dd')
                            if return_dt and return_dd:
                                goods_info["return_info"]["title"] = return_dt.get_text(strip=True)
                                goods_info["return_info"]["description"] = return_dd.get_text(strip=True)
                
                # ìƒí’ˆ ì˜µì…˜ ì¶”ì¶œ
                option_info = info_area.find('div', {'id': re.compile(r'OptionInfo')})
                if option_info:
                    # ë³¸ì²´ ì»¬ëŸ¬ (inventory) - stock í´ë˜ìŠ¤ê°€ ìˆëŠ” dl ì°¾ê¸°
                    all_dls = option_info.find_all('dl', {'class': 'detailsArea'})
                    inventory_dl = None
                    for dl in all_dls:
                        if 'stock' in dl.get('class', []):
                            inventory_dl = dl
                            break
                    if not inventory_dl:
                        # stock í´ë˜ìŠ¤ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ detailsArea dl ì‚¬ìš©
                        inventory_dl = option_info.find('dl', {'class': 'detailsArea'})
                    if inventory_dl:
                        inventory_dt = inventory_dl.find('dt')
                        if inventory_dt:
                            option_name = inventory_dt.get_text(strip=True)
                            inventory_outer = inventory_dl.find('div', {'id': re.compile(r'inventory_outer')})
                            if inventory_outer:
                                content_inventory = inventory_outer.find('ul', {'id': re.compile(r'content_inventory')})
                                if content_inventory:
                                    option_items = content_inventory.find_all('li')
                                    option_values = []
                                    for item in option_items:
                                        span = item.find('span')
                                        if span:
                                            option_text = span.get_text(strip=True)
                                            if option_text and option_text != '-----------------------------------------------------------':
                                                option_values.append(option_text)
                                    if option_values:
                                        goods_info["options"].append({
                                            "name": option_name,
                                            "type": "inventory",
                                            "values": option_values
                                        })
                    
                    # ì´ë¼ìŠ¤íŠ¸ (opt)
                    opt_dls = option_info.find_all('dl', {'class': 'detailsArea'})
                    for opt_dl in opt_dls:
                        opt_dt = opt_dl.find('dt')
                        if opt_dt:
                            opt_name = opt_dt.get_text(strip=True)
                            # inventoryê°€ ì•„ë‹Œ ê²½ìš°ë§Œ (ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ)
                            if 'ë³¸ì²´ ì»¬ëŸ¬' not in opt_name and 'âˆ™' not in opt_name:
                                opt_outer = opt_dl.find('div', {'id': re.compile(r'opt_outer')})
                                if opt_outer:
                                    content_opt = opt_outer.find('ul', {'id': re.compile(r'content_opt')})
                                    if content_opt:
                                        opt_items = content_opt.find_all('li')
                                        opt_values = []
                                        for item in opt_items:
                                            span = item.find('span')
                                            if span:
                                                opt_text = span.get_text(strip=True)
                                                if opt_text and opt_text != '-----------------------------------------------------------':
                                                    opt_values.append(opt_text)
                                        if opt_values:
                                            goods_info["options"].append({
                                                "name": opt_name,
                                                "type": "option",
                                                "values": opt_values
                                            })
                    
                    # ì‘ì„± ì´ë¦„ í•„ë“œ
                    request_info = option_info.find('span', {'id': re.compile(r'RequestInfo')})
                    if request_info:
                        request_dl = request_info.find('dl', {'class': 'detailsArea'})
                        if request_dl:
                            request_dt = request_dl.find('dt')
                            request_dd = request_dl.find('dd')
                            if request_dt and request_dd:
                                field_name = request_dt.get_text(strip=True)
                                input_field = request_dd.find('input')
                                if input_field:
                                    maxlength = input_field.get('maxlength', '')
                                    goods_info["custom_name_field"] = {
                                        "name": field_name,
                                        "maxlength": maxlength
                                    }
        
        # #region agent log
        _log_debug("debug-session", "run1", "D", "crawler.py:_extract_goods_info", "ìƒí’ˆ ì •ë³´ ì˜ì—­ ì¶”ì¶œ ì™„ë£Œ", {
            "has_images": len(goods_info.get("product_images", [])) > 0,
            "has_brand": bool(goods_info.get("brand")),
            "has_title": bool(goods_info.get("product_title")),
            "has_price": bool(goods_info.get("price_info")),
            "options_count": len(goods_info.get("options", [])),
            "is_empty": not goods_info.get("product_title") and not goods_info.get("price_info")
        })
        # #endregion
        
        return goods_info
    
    def _extract_seller_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """íŒë§¤ì ì •ë³´ ì¶”ì¶œ"""
        seller_info = {
            "shop_id": None,
            "shop_name": None,
            "shop_level": None
        }
        
        # Shop ë§í¬ì—ì„œ ì¶”ì¶œ
        shop_link = soup.find('a', href=re.compile(r'/shop/'))
        if shop_link:
            href = shop_link.get('href', '')
            match = re.search(r'/shop/([^/?]+)', href)
            if match:
                seller_info["shop_id"] = match.group(1)
            seller_info["shop_name"] = shop_link.get_text(strip=True)
        
        return seller_info
    
    def _extract_shipping_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ë°°ì†¡ ì •ë³´ ì¶”ì¶œ - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        # #region agent log
        _log_debug("debug-session", "run1", "F", "crawler.py:_extract_shipping_info", "ë°°ì†¡ ì •ë³´ ì¶”ì¶œ ì‹œì‘", {})
        # #endregion

        shipping_info: Dict[str, Any] = {
            "shipping_fee": None,
            "shipping_method": None,
            "estimated_delivery": None,
            "free_shipping": False,  # ë¬´ë£Œë°°ì†¡ ì—¬ë¶€
            "return_policy": None,  # ë°˜í’ˆ ì •ì±… ì •ë³´
        }

        # ë°°ì†¡ë¹„ ì •ë³´ ì°¾ê¸° (ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„)
        shipping_patterns = [
            r'é€æ–™[ï¼š:]\s*(\d+)å††',
            r'é€æ–™[ï¼š:]\s*ç„¡æ–™',
            r'é€æ–™[ï¼š:]\s*FREE',
            r'é…é€æ–™[ï¼š:]\s*(\d+)å††',
            r'ë°°ì†¡ë¹„[ï¼š:]\s*(\d+)å††',
            r'Shipping[ï¼š:]\s*(\d+)å††',
        ]

        # ë°°ì†¡ ê´€ë ¨ í…ìŠ¤íŠ¸ ì°¾ê¸° (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        shipping_pattern = self._create_jp_kr_regex("é€æ–™", "ë°°ì†¡ë¹„")
        delivery_pattern = self._create_jp_kr_regex("é…é€", "ë°°ì†¡")
        shipping_elem = soup.find(string=re.compile(f'{shipping_pattern}|{delivery_pattern}|Shipping', re.I))
        if shipping_elem:
            parent = shipping_elem.find_parent()
            if parent:
                shipping_text = parent.get_text(strip=True)

                # ë¬´ë£Œë°°ì†¡ í™•ì¸ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘)
                free_shipping_pattern = self._create_jp_kr_regex("é€æ–™ç„¡æ–™", "ë¬´ë£Œë°°ì†¡")
                if (
                    re.search(free_shipping_pattern, shipping_text)
                    or 'FREE' in shipping_text.upper()
                    or 'ç„¡æ–™' in shipping_text
                    or 'ë¬´ë£Œ' in shipping_text
                ):
                    shipping_info["free_shipping"] = True
                    shipping_info["shipping_fee"] = 0
                else:
                    # ìˆ«ì ì¶”ì¶œ
                    for pattern in shipping_patterns:
                        m = re.search(pattern, shipping_text)
                        if m and m.group(1):
                            shipping_info["shipping_fee"] = int(m.group(1))
                            break

                    # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìˆ«ìë§Œ ì¶”ì¶œ
                    if shipping_info["shipping_fee"] is None:
                        numbers = re.findall(r'\d+', shipping_text)
                        if numbers:
                            shipping_info["shipping_fee"] = int(numbers[0])

        # ë°˜í’ˆ ì •ì±… ì •ë³´ ì¶”ì¶œ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›) - ê°•í™”ëœ ì¶”ì¶œ
        return_pattern = self._create_jp_kr_regex("è¿”å“", "ë°˜í’ˆ")
        return_elem = soup.find(string=re.compile(f'{return_pattern}|è¿”å´|Return', re.I))

        # ë°˜í’ˆ ì •ë³´ê°€ ìˆëŠ” ì„¹ì…˜ ì°¾ê¸° (ë” ë„“ì€ ë²”ìœ„)
        return_section = None
        if return_elem:
            return_section = return_elem.find_parent()
            if return_section:
                parent = return_section.find_parent()
                if parent is not None:
                    return_section = parent

        # ë°˜í’ˆ ê´€ë ¨ ì„ íƒìë¡œë„ ì‹œë„
        if return_section is None:
            return_selectors = [
                'div[class*="è¿”å“"]',
                'div[class*="ë°˜í’ˆ"]',
                'div[class*="return"]',
                '[id*="è¿”å“"]',
                '[id*="ë°˜í’ˆ"]',
                '[id*="return"]',
            ]
            for selector in return_selectors:
                return_section = soup.select_one(selector)
                if return_section:
                    break

        if return_section is not None:
            return_text = return_section.get_text()
            # "è¿”å“ç„¡æ–™" ë˜ëŠ” "ë¬´ë£Œë°˜í’ˆ" ë˜ëŠ” "è¿”å“ç„¡æ–™ã‚µãƒ¼ãƒ“ã‚¹" í™•ì¸
            free_return_pattern = self._create_jp_kr_regex("è¿”å“ç„¡æ–™", "ë¬´ë£Œë°˜í’ˆ")
            if (
                re.search(free_return_pattern, return_text)
                or 'ç„¡æ–™è¿”å“' in return_text
                or 'è¿”å“ç„¡æ–™ã‚µãƒ¼ãƒ“ã‚¹' in return_text
            ):
                shipping_info["return_policy"] = "free_return"
            elif re.search(return_pattern, return_text):
                shipping_info["return_policy"] = "return_available"

        # ì¶”ê°€ ì‹œë„: í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë°˜í’ˆ ì •ë³´ ì°¾ê¸°
        if not shipping_info["return_policy"]:
            all_text = soup.get_text()
            free_return_pattern = self._create_jp_kr_regex("è¿”å“ç„¡æ–™", "ë¬´ë£Œë°˜í’ˆ")
            if (
                re.search(free_return_pattern, all_text)
                or 'ç„¡æ–™è¿”å“' in all_text
                or 'è¿”å“ç„¡æ–™ã‚µãƒ¼ãƒ“ã‚¹' in all_text
            ):
                shipping_info["return_policy"] = "free_return"
            elif re.search(return_pattern, all_text):
                shipping_info["return_policy"] = "return_available"

        # #region agent log
        _log_debug("debug-session", "run1", "F", "crawler.py:_extract_shipping_info", "ë°°ì†¡ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ", {
            "shipping_fee": shipping_info.get("shipping_fee"),
            "free_shipping": shipping_info.get("free_shipping"),
            "shipping_method": shipping_info.get("shipping_method"),
            "estimated_delivery": shipping_info.get("estimated_delivery"),
            "is_empty": not shipping_info.get("shipping_fee") and not shipping_info.get("free_shipping"),
        })
        # #endregion

        return shipping_info
    
    def _extract_move_product(self, soup: BeautifulSoup) -> bool:
        """MOVE ìƒí’ˆ ì—¬ë¶€ ì¶”ì¶œ - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        # MOVE ê´€ë ¨ í…ìŠ¤íŠ¸ ì°¾ê¸°
        move_indicators = [
            soup.find(string=re.compile(r'MOVE|ãƒ ãƒ¼ãƒ–', re.I)),
            soup.find('span', string=re.compile(r'MOVE|ãƒ ãƒ¼ãƒ–', re.I)),
            soup.find('div', string=re.compile(r'MOVE|ãƒ ãƒ¼ãƒ–', re.I)),
            soup.find('a', href=re.compile(r'/move/', re.I)),
        ]
        
        for indicator in move_indicators:
            if indicator:
                return True
        
        # URLì—ì„œ MOVE í™•ì¸
        move_link = soup.find('a', href=re.compile(r'/move/', re.I))
        if move_link:
            return True
        
        # í´ë˜ìŠ¤ëª…ì—ì„œ MOVE í™•ì¸
        move_elem = soup.find(class_=re.compile(r'move', re.I))
        if move_elem:
            return True
        
        return False
    def _extract_qpoint_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ìƒì„¸ ì¶”ì¶œ - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        # #region agent log
        _log_debug("debug-session", "run1", "I", "crawler.py:_extract_qpoint_info", "Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ì¶”ì¶œ ì‹œì‘", {})
        # #endregion

        qpoint_info: Dict[str, Any] = {
            "max_points": None,
            "receive_confirmation_points": None,
            "review_points": None,
            "auto_points": None,
        }

        # Qãƒã‚¤ãƒ³ãƒˆ ì„¹ì…˜ ì°¾ê¸° (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        qpoint_selectors = [
            'div[class*="qpoint"]',
            'div[class*="ãƒã‚¤ãƒ³ãƒˆ"]',
            'table[class*="qpoint"]',
            'table[class*="ãƒã‚¤ãƒ³ãƒˆ"]',
            '.qpoint-info',
            '#qpoint-info',
            '[id*="qpoint"]',
            '[id*="ãƒã‚¤ãƒ³ãƒˆ"]',
        ]

        qpoint_section = None
        for selector in qpoint_selectors:
            qpoint_section = soup.select_one(selector)
            if qpoint_section:
                break

        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        if qpoint_section is None:
            qpoint_method_pattern = self._create_jp_kr_regex("Qãƒã‚¤ãƒ³ãƒˆç²å¾—æ–¹æ³•", "Qí¬ì¸íŠ¸íšë“ë°©ë²•")
            qpoint_get_pattern = self._create_jp_kr_regex("Qãƒã‚¤ãƒ³ãƒˆç²å¾—", "Qí¬ì¸íŠ¸íšë“")
            qpoint_pattern = self._create_jp_kr_regex("Qãƒã‚¤ãƒ³ãƒˆ", "Qí¬ì¸íŠ¸")
            qpoint_text_elem = soup.find(
                string=re.compile(f'{qpoint_method_pattern}|{qpoint_get_pattern}|{qpoint_pattern}', re.I)
            )
            if qpoint_text_elem:
                qpoint_section = qpoint_text_elem.find_parent()
                if qpoint_section:
                    parent = qpoint_section.find_parent()
                    if parent is not None:
                        qpoint_section = parent

        if qpoint_section is not None:
            # ì„¹ì…˜ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            qpoint_text = qpoint_section.get_text()

            # "å—å–ç¢ºèª: æœ€å¤§1P" ë˜ëŠ” "ìˆ˜ë ¹í™•ì¸: ìµœëŒ€1P" íŒ¨í„´ ì°¾ê¸° (ë” ìœ ì—°í•œ íŒ¨í„´)
            receive_pattern = self._create_jp_kr_regex("å—å–ç¢ºèª", "ìˆ˜ë ¹í™•ì¸")
            max_pattern = self._create_jp_kr_regex("æœ€å¤§", "ìµœëŒ€")

            # íŒ¨í„´ 1: "å—å–ç¢ºèª: æœ€å¤§1P"
            receive_match = re.search(
                f'{receive_pattern}[ï¼š:\s]*{max_pattern}?\s*(\d+)P', qpoint_text, re.I
            )
            if receive_match:
                try:
                    points = int(receive_match.group(1))
                    qpoint_info["receive_confirmation_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            # íŒ¨í„´ 2: "å—å–ç¢ºèª.*(\d+)P" (ë” ìœ ì—°í•œ íŒ¨í„´, ìˆ«ìë§Œ ë§¤ì¹­)
            if not qpoint_info["receive_confirmation_points"]:
                receive_match2 = re.search(
                    f'{receive_pattern}[ï¼š:\s]*.*?(\d+)\s*P', qpoint_text, re.I
                )
                if receive_match2:
                    try:
                        points = int(receive_match2.group(1))
                        qpoint_info["receive_confirmation_points"] = points
                    except (ValueError, AttributeError):
                        pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            # "ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ: æœ€å¤§20P" ë˜ëŠ” "ë¦¬ë·°ì‘ì„±: ìµœëŒ€20P" íŒ¨í„´ ì°¾ê¸°
            review_create_pattern = self._create_jp_kr_regex("ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ", "ë¦¬ë·°ì‘ì„±")

            # íŒ¨í„´ 1: "ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ: æœ€å¤§20P"
            review_match = re.search(
                f'{review_create_pattern}[ï¼š:\s]*{max_pattern}?\s*(\d+)P',
                qpoint_text,
                re.I,
            )
            if review_match:
                try:
                    points = int(review_match.group(1))
                    qpoint_info["review_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            # íŒ¨í„´ 2: "ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ.*(\d+)P" (ë” ìœ ì—°í•œ íŒ¨í„´, ìˆ«ìë§Œ ë§¤ì¹­)
            if not qpoint_info["review_points"]:
                review_match2 = re.search(
                    f'{review_create_pattern}[ï¼š:\s]*.*?(\d+)\s*P', qpoint_text, re.I
                )
                if review_match2:
                    try:
                        points = int(review_match2.group(1))
                        qpoint_info["review_points"] = points
                    except (ValueError, AttributeError):
                        pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            # "æœ€å¤§(\d+)P" ë˜ëŠ” "ìµœëŒ€(\d+)P" íŒ¨í„´ ì°¾ê¸° (ì „ì²´ ìµœëŒ€ í¬ì¸íŠ¸)
            max_match = re.search(f'{max_pattern}\s*(\d+)\s*P', qpoint_text, re.I)
            if max_match:
                try:
                    points = int(max_match.group(1))
                    qpoint_info["max_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            # "é…é€å®Œäº†.*è‡ªå‹•.*(\d+)P" ë˜ëŠ” "ë°°ì†¡ì™„ë£Œ.*ìë™.*(\d+)P" íŒ¨í„´ ì°¾ê¸° (ìë™ í¬ì¸íŠ¸)
            delivery_complete_pattern = self._create_jp_kr_regex("é…é€å®Œäº†", "ë°°ì†¡ì™„ë£Œ")
            auto_pattern = self._create_jp_kr_regex("è‡ªå‹•", "ìë™")
            auto_match = re.search(
                f'{delivery_complete_pattern}.*?{auto_pattern}.*?(\d+)\s*P',
                qpoint_text,
                re.I | re.DOTALL,
            )
            if auto_match:
                try:
                    points = int(auto_match.group(1))
                    qpoint_info["auto_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

        # ì¶”ê°€ ì‹œë„: í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ì°¾ê¸°
        if not any(qpoint_info.values()):
            all_text = soup.get_text()
            receive_pattern = self._create_jp_kr_regex("å—å–ç¢ºèª", "ìˆ˜ë ¹í™•ì¸")
            review_create_pattern = self._create_jp_kr_regex("ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæˆ", "ë¦¬ë·°ì‘ì„±")
            max_pattern = self._create_jp_kr_regex("æœ€å¤§", "ìµœëŒ€")

            receive_match = re.search(
                f'{receive_pattern}[ï¼š:\s]*.*?(\d+)\s*P', all_text, re.I
            )
            if receive_match:
                try:
                    points = int(receive_match.group(1))
                    qpoint_info["receive_confirmation_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            review_match = re.search(
                f'{review_create_pattern}[ï¼š:\s]*.*?(\d+)\s*P', all_text, re.I
            )
            if review_match:
                try:
                    points = int(review_match.group(1))
                    qpoint_info["review_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

            max_match = re.search(f'{max_pattern}\s*(\d+)\s*P', all_text, re.I)
            if max_match:
                try:
                    points = int(max_match.group(1))
                    qpoint_info["max_points"] = points
                except (ValueError, AttributeError):
                    pass  # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

        # #region agent log
        _log_debug("debug-session", "run1", "I", "crawler.py:_extract_qpoint_info", "Qãƒã‚¤ãƒ³ãƒˆ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ", {
            "max_points": qpoint_info.get("max_points"),
            "receive_confirmation_points": qpoint_info.get("receive_confirmation_points"),
            "review_points": qpoint_info.get("review_points"),
            "auto_points": qpoint_info.get("auto_points"),
            "is_empty": not any(qpoint_info.values()),
        })
        # #endregion

        return qpoint_info
    
    def _extract_coupon_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì¿ í° ìƒì„¸ ì •ë³´ ì¶”ì¶œ - ì‹¤ì œ Qoo10 í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        # #region agent log
        _log_debug("debug-session", "run1", "G", "crawler.py:_extract_coupon_info", "ì¿ í° ì •ë³´ ì¶”ì¶œ ì‹œì‘", {})
        # #endregion
        
        coupon_info = {
            "has_coupon": False,
            "coupon_discount": None,
            "coupon_type": None,  # "shop_favorite", "password", "auto" ë“±
            "max_discount": None,
            "coupon_text": None
        }
        
        # ì¿ í° ì„¹ì…˜ ì°¾ê¸° (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        coupon_discount_pattern = self._create_jp_kr_regex("ã‚¯ãƒ¼ãƒãƒ³å‰²å¼•", "ì¿ í°í• ì¸")
        coupon_pattern = self._create_jp_kr_regex("ã‚¯ãƒ¼ãƒãƒ³", "ì¿ í°")
        discount_pattern = self._create_jp_kr_regex("å‰²å¼•", "í• ì¸")
        shop_favorite_pattern = self._create_jp_kr_regex("ã‚·ãƒ§ãƒƒãƒ—ãŠæ°—ã«å…¥ã‚Šå‰²å¼•", "ìƒµì¦ê²¨ì°¾ê¸°í• ì¸")
        coupon_section = soup.find(string=re.compile(f'{coupon_discount_pattern}|{coupon_pattern}|{discount_pattern}|{shop_favorite_pattern}', re.I))
        if coupon_section:
            parent = coupon_section.find_parent()
            if parent:
                coupon_info["has_coupon"] = True
                coupon_text = parent.get_text()
                coupon_info["coupon_text"] = coupon_text
                
                # "ãƒ—ãƒ©ã‚¹(\d+)å‰²å¼•" ë˜ëŠ” "í”ŒëŸ¬ìŠ¤(\d+)í• ì¸" ë˜ëŠ” "æœ€å¤§(\d+)å††" ë˜ëŠ” "ìµœëŒ€(\d+)ì—”" íŒ¨í„´ ì°¾ê¸°
                plus_pattern = self._create_jp_kr_regex("ãƒ—ãƒ©ã‚¹", "í”ŒëŸ¬ìŠ¤")
                max_pattern = self._create_jp_kr_regex("æœ€å¤§", "ìµœëŒ€")
                discount_match = re.search(f'{plus_pattern}(\d+){discount_pattern}|{max_pattern}(\d+)å††', coupon_text)
                if discount_match:
                    discount = discount_match.group(1) or discount_match.group(2)
                    coupon_info["max_discount"] = int(discount) if discount.isdigit() else None
                
                # ì¿ í° íƒ€ì… í™•ì¸ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
                shop_favorite_pattern = self._create_jp_kr_regex("ã‚·ãƒ§ãƒƒãƒ—ãŠæ°—ã«å…¥ã‚Š", "ìƒµì¦ê²¨ì°¾ê¸°")
                favorite_register_pattern = self._create_jp_kr_regex("ãŠæ°—ã«å…¥ã‚Šç™»éŒ²", "ì¦ê²¨ì°¾ê¸°ë“±ë¡")
                if re.search(shop_favorite_pattern, coupon_text) or re.search(favorite_register_pattern, coupon_text):
                    coupon_info["coupon_type"] = "shop_favorite"
                elif 'ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰' in coupon_text or 'password' in coupon_text.lower() or 'ë¹„ë°€ë²ˆí˜¸' in coupon_text:
                    coupon_info["coupon_type"] = "password"
                else:
                    coupon_info["coupon_type"] = "auto"
        
        # #region agent log
        _log_debug("debug-session", "run1", "G", "crawler.py:_extract_coupon_info", "ì¿ í° ì •ë³´ ì¶”ì¶œ ì™„ë£Œ", {
            "has_coupon": coupon_info.get("has_coupon"),
            "coupon_type": coupon_info.get("coupon_type"),
            "max_discount": coupon_info.get("max_discount"),
            "is_empty": not coupon_info.get("has_coupon")
        })
        # #endregion
        
        return coupon_info
    
    def _extract_page_structure(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """
        í˜ì´ì§€ êµ¬ì¡° ë° ëª¨ë“  div class ì¶”ì¶œ (ìµœì í™” ë²„ì „)
        Qoo10 í˜ì´ì§€ì˜ ëª¨ë“  div classë¥¼ ë¶„ì„í•˜ì—¬ ê° ìš”ì†Œì˜ ì˜ë¯¸ë¥¼ íŒŒì•…
        """
        # ì—ëŸ¬ í˜ì´ì§€ ê°ì§€ (ë¡œê¹… ì „ì— ìˆ˜í–‰)
        is_error_page = False
        error_indicators = []
        if soup:
            # ì—ëŸ¬ í˜ì´ì§€ í´ë˜ìŠ¤ í™•ì¸
            error_classes = soup.find_all(class_=re.compile(r'error|Error|ERROR|section_error'))
            if error_classes:
                is_error_page = True
                error_indicators.append("error_class_found")
            
            # ì—ëŸ¬ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ í™•ì¸
            page_text = soup.get_text().lower() if soup else ""
            error_keywords = ["ã‚¨ãƒ©ãƒ¼", "error", "ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "not found", "404", "500", "ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“"]
            if any(keyword in page_text for keyword in error_keywords):
                is_error_page = True
                error_indicators.append("error_text_found")
            
            # HTML ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ì—ëŸ¬ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)
            html_str = str(soup)
            if len(html_str) < 5000:  # 5KB ë¯¸ë§Œì´ë©´ ì˜ì‹¬
                is_error_page = True
                error_indicators.append("html_too_short")
        
        # #region agent log - H1, H3, H4 ê°€ì„¤ ê²€ì¦
        import json
        from datetime import datetime
        log_path = "/Users/chunghyo/qoo10-ai-agent/.cursor/debug.log"
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "id": f"log_{int(datetime.now().timestamp() * 1000)}_extract_start",
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "location": "crawler.py:_extract_page_structure",
                    "message": "í˜ì´ì§€ êµ¬ì¡° ì¶”ì¶œ ì‹œì‘",
                    "data": {
                        "soup_exists": soup is not None,
                        "is_error_page": is_error_page,
                        "error_indicators": error_indicators,
                        "html_length": len(str(soup)) if soup else 0
                    },
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "H1,H3,H4"
                }, ensure_ascii=False) + "\n")
        except: pass
        # #endregion
        
        # ì—ëŸ¬ í˜ì´ì§€ì¸ ê²½ìš° ë¹ˆ êµ¬ì¡° ë°˜í™˜
        if is_error_page:
            return {
                "all_div_classes": [],
                "class_frequency": {},
                "key_elements": {},
                "semantic_structure": {},
                "is_error_page": True,
                "error_indicators": error_indicators
            }
        
        structure = {
            "all_div_classes": [],
            "class_frequency": {},
            "key_elements": {},
            "semantic_structure": {}
        }
        
        # ìµœì í™”: í•œ ë²ˆì˜ ìˆœíšŒë¡œ ëª¨ë“  ì •ë³´ ìˆ˜ì§‘
        all_divs = soup.find_all('div', limit=1000)  # ìµœëŒ€ 1000ê°œë¡œ ì œí•œ
        
        # #region agent log - H1 ê°€ì„¤ ê²€ì¦
        try:
            # í˜ì´ì§€ ì „ì²´ êµ¬ì¡° í™•ì¸
            all_elements = soup.find_all()
            body_elements = soup.find_all('body')
            html_length = len(str(soup)) if soup else 0
            
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "id": f"log_{int(datetime.now().timestamp() * 1000)}_divs_found",
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "location": "crawler.py:_extract_page_structure",
                    "message": "div ìš”ì†Œ ê°œìˆ˜ í™•ì¸",
                    "data": {
                        "div_count": len(all_divs),
                        "total_elements": len(all_elements),
                        "body_elements": len(body_elements),
                        "html_length": html_length,
                        "sample_div_classes": [div.get('class', [])[:3] for div in all_divs[:5]] if all_divs else []
                    },
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "H1"
                }, ensure_ascii=False) + "\n")
        except Exception as e:
            try:
                with open(log_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps({
                        "id": f"log_{int(datetime.now().timestamp() * 1000)}_divs_found_error",
                        "timestamp": int(datetime.now().timestamp() * 1000),
                        "location": "crawler.py:_extract_page_structure",
                        "message": "div ìš”ì†Œ ê°œìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜",
                        "data": {"error": str(e)},
                        "sessionId": "debug-session",
                        "runId": "run1",
                        "hypothesisId": "H1"
                    }, ensure_ascii=False) + "\n")
            except: pass
        # #endregion
        
        # ì£¼ìš” ìš”ì†Œë³„ë¡œ ë¶„ë¥˜í•  íŒ¨í„´ ì •ì˜
        key_patterns = {
            "product_info": ["product", "goods", "item", "detail", "info", "name", "title"],
            "price_info": ["price", "cost", "discount", "sale", "original", "prc"],
            "image_info": ["image", "img", "photo", "thumbnail", "thmb", "picture"],
            "review_info": ["review", "rating", "star", "comment", "evaluation"],
            "seller_info": ["shop", "seller", "store", "vendor", "merchant"],
            "shipping_info": ["shipping", "delivery", "ship", "é…é€", "é€æ–™"],
            "coupon_info": ["coupon", "discount", "å‰²å¼•", "ã‚¯ãƒ¼ãƒãƒ³"],
            "qpoint_info": ["qpoint", "point", "ãƒã‚¤ãƒ³ãƒˆ", "Qãƒã‚¤ãƒ³ãƒˆ"],
        }
        
        # ì˜ë¯¸ ìˆëŠ” êµ¬ì¡° ìš”ì†Œë¥¼ ìœ„í•œ íƒœê·¸ ë§¤í•‘
        semantic_mapping = {
            "product_name_elements": ["name", "title", "goods_name", "product_name"],
            "price_elements": ["price", "prc", "cost"],
            "image_elements": ["image", "img", "photo", "thmb", "thumbnail"],
            "description_elements": ["description", "detail", "content"],
            "review_elements": ["review", "rating", "star", "comment"],
            "seller_elements": ["shop", "seller", "store"],
            "shipping_elements": ["shipping", "ship", "delivery", "é…é€", "é€æ–™"],
            "coupon_elements": ["coupon", "å‰²å¼•", "ã‚¯ãƒ¼ãƒãƒ³", "discount"],
            "qpoint_elements": ["qpoint", "point", "ãƒã‚¤ãƒ³ãƒˆ"],
        }
        
        # í•œ ë²ˆì˜ ìˆœíšŒë¡œ ëª¨ë“  ì •ë³´ ìˆ˜ì§‘
        semantic_elements = {key: [] for key in semantic_mapping.keys()}
        seen_classes = set()
        
        for div in all_divs:
            classes = div.get('class', [])
            if not isinstance(classes, list):
                continue
                
            for cls in classes:
                if not cls or cls in seen_classes:
                    continue
                    
                seen_classes.add(cls)
                structure["all_div_classes"].append(cls)
                structure["class_frequency"][cls] = structure["class_frequency"].get(cls, 0) + 1
                
                cls_lower = cls.lower()
                
                # ì£¼ìš” ìš”ì†Œ ë¶„ë¥˜
                for category, patterns in key_patterns.items():
                    if any(pattern in cls_lower for pattern in patterns):
                        if category not in structure["key_elements"]:
                            structure["key_elements"][category] = []
                        structure["key_elements"][category].append({
                            "class": cls,
                            "frequency": structure["class_frequency"][cls]
                        })
                
                # ì˜ë¯¸ ìˆëŠ” êµ¬ì¡° ìš”ì†Œ ë¶„ë¥˜
                for semantic_key, keywords in semantic_mapping.items():
                    if any(keyword in cls_lower for keyword in keywords):
                        semantic_elements[semantic_key].append(cls)
        
        # ì¶”ê°€ë¡œ íŠ¹ì • íƒœê·¸ì—ì„œë„ ìˆ˜ì§‘ (ìµœì í™”: ì œí•œëœ ì„ íƒìë§Œ ì‚¬ìš©)
        quick_selectors = {
            "product_name_elements": ['h1', 'h2[class*="name"]'],
            "price_elements": ['span[class*="price"]', 'strong[class*="price"]'],
            "image_elements": ['img[class*="thumbnail"]'],
        }
        
        for semantic_key, selectors in quick_selectors.items():
            for selector in selectors[:2]:  # ìµœëŒ€ 2ê°œ ì„ íƒìë§Œ ì‹œë„
                try:
                    elems = soup.select(selector, limit=10)  # ìµœëŒ€ 10ê°œë§Œ
                    for elem in elems:
                        classes = elem.get('class', [])
                        if classes:
                            for cls in classes:
                                if cls and cls not in seen_classes:
                                    semantic_elements[semantic_key].append(cls)
                                    seen_classes.add(cls)
                except:
                    continue  # ì„ íƒì ì˜¤ë¥˜ ë¬´ì‹œ
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ ê³„ì‚°
        for key in semantic_elements:
            class_counts = {}
            for cls in semantic_elements[key]:
                class_counts[cls] = class_counts.get(cls, 0) + 1
            semantic_elements[key] = [
                {"class": cls, "frequency": count}
                for cls, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:20]  # ìƒìœ„ 20ê°œë§Œ
            ]
        
        structure["semantic_structure"] = semantic_elements
        
        # ê³ ìœ í•œ class ëª©ë¡ ì •ë¦¬ (ìµœëŒ€ 500ê°œë¡œ ì œí•œ)
        structure["all_div_classes"] = sorted(list(set(structure["all_div_classes"])))[:500]
        
        # #region agent log - H1, H3, H4 ê°€ì„¤ ê²€ì¦
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps({
                    "id": f"log_{int(datetime.now().timestamp() * 1000)}_extract_end",
                    "timestamp": int(datetime.now().timestamp() * 1000),
                    "location": "crawler.py:_extract_page_structure",
                    "message": "í˜ì´ì§€ êµ¬ì¡° ì¶”ì¶œ ì™„ë£Œ",
                    "data": {
                        "total_classes": len(structure["all_div_classes"]),
                        "key_elements_count": len(structure["key_elements"]),
                        "semantic_structure_keys": list(structure["semantic_structure"].keys()),
                        "key_elements_keys": list(structure["key_elements"].keys()),
                        "semantic_structure_sample": {k: len(v) for k, v in structure["semantic_structure"].items()}
                    },
                    "sessionId": "debug-session",
                    "runId": "run1",
                    "hypothesisId": "H1,H3,H4"
                }, ensure_ascii=False) + "\n")
        except: pass
        # #endregion
        
        return structure
    
    def _extract_product_keywords(self, product_name: str) -> List[str]:
        """ìƒí’ˆëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)

        - ëŒ€ì†Œë¬¸ì/í•œê¸€/ì¼ë³¸ì–´ ë‹¨ì–´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ
        - í–¥í›„ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶™ì¼ ìˆ˜ ìˆë„ë¡ ìµœì†Œí•œì˜ êµ¬í˜„ë§Œ ìœ ì§€
        """
        keywords = []
        
        # ìƒí’ˆëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ (2ì ì´ìƒ í† í°)
        words = re.findall(r"\b\w+\b", product_name, re.UNICODE)
        for word in words:
            if len(word) >= 2:
                keywords.append(word)
        
        return keywords[:10]  # ìµœëŒ€ 10ê°œ
    
    def _extract_shop_coupons(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """Shop ì¿ í° ì •ë³´ ì¶”ì¶œ - ì‹¤ì œ Qoo10 Shop í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        coupons = []
        
        # ì‹¤ì œ Qoo10 Shop í˜ì´ì§€ êµ¬ì¡°: ì¿ í° ë¦¬ìŠ¤íŠ¸
        # ìš°ì„ ìˆœìœ„ 1: ì¿ í° ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ ì°¾ê¸°
        coupon_list = soup.select('.coupon-list, [class*="coupon"], [class*="ã‚¯ãƒ¼ãƒãƒ³"]')
        
        # ìš°ì„ ìˆœìœ„ 2: ê°œë³„ ì¿ í° ìš”ì†Œ ì°¾ê¸°
        coupon_selectors = [
            '.coupon-item',
            '.discount-coupon',
            'div[class*="coupon"]',
            'li[class*="coupon"]',
            '[class*="ã‚¯ãƒ¼ãƒãƒ³"]',
            '[class*="å‰²å¼•"]',
            'div[class*="off"]'  # "5%off", "10%off" ê°™ì€ íŒ¨í„´
        ]
        
        seen_coupons = set()
        
        # ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì¿ í° íŒ¨í„´ ì°¾ê¸°
        page_text = soup.get_text()
        
        # íŒ¨í„´ 1: "5,000å††ä»¥ä¸Šã®ã”è³¼å…¥ã§10%off" ë˜ëŠ” "5,000ì—” ì´ìƒêµ¬ë§¤ì‹œ 10%off" ê°™ì€ í˜•ì‹ (ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›)
        above_pattern = self._create_jp_kr_regex("ä»¥ä¸Š", "ì´ìƒ")
        discount_pattern = self._create_jp_kr_regex("å‰²å¼•", "í• ì¸")
        coupon_patterns = [
            f'(\\d{{1,3}}(?:,\\d{{3}})*)å††{above_pattern}.*?(\\d+)%off',  # ê¸ˆì•¡ ì´ìƒ êµ¬ë§¤ ì‹œ í• ì¸ìœ¨
            f'(\\d{{1,3}}(?:,\\d{{3}})*)å††{above_pattern}.*?(\\d+)%{discount_pattern}',  # ê¸ˆì•¡ ì´ìƒ êµ¬ë§¤ ì‹œ í• ì¸ìœ¨ (ì¼ë³¸ì–´-í•œêµ­ì–´)
            f'(\\d+)%off.*?(\\d{{1,3}}(?:,\\d{{3}})*)å††{above_pattern}',  # í• ì¸ìœ¨ + ìµœì†Œ ê¸ˆì•¡
            f'(\\d+)%{discount_pattern}.*?(\\d{{1,3}}(?:,\\d{{3}})*)å††{above_pattern}',  # í• ì¸ìœ¨ + ìµœì†Œ ê¸ˆì•¡ (ì¼ë³¸ì–´-í•œêµ­ì–´)
        ]
        
        for pattern in coupon_patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ í‚¤ ìƒì„±
                coupon_key = f"{match.group(1)}_{match.group(2)}"
                if coupon_key not in seen_coupons:
                    seen_coupons.add(coupon_key)
                    
                    # ì²« ë²ˆì§¸ ê·¸ë£¹ì´ ê¸ˆì•¡ì¸ì§€ í• ì¸ìœ¨ì¸ì§€ í™•ì¸
                    try:
                        if 'å††' in match.group(1) or ',' in match.group(1):
                            # ì²« ë²ˆì§¸ ê·¸ë£¹ì´ ê¸ˆì•¡
                            min_amount = self._parse_price(match.group(1))
                            discount_rate = int(match.group(2)) if match.group(2).isdigit() else 0
                        else:
                            # ì²« ë²ˆì§¸ ê·¸ë£¹ì´ í• ì¸ìœ¨
                            discount_rate = int(match.group(1)) if match.group(1).isdigit() else 0
                            min_amount = self._parse_price(match.group(2))
                        
                        if discount_rate > 0 or min_amount > 0:
                            coupon = {
                                "discount_rate": discount_rate,
                                "min_amount": min_amount,
                                "valid_until": None,
                                "description": match.group(0)
                            }
                            coupons.append(coupon)
                    except (ValueError, AttributeError):
                        continue
        
        # íŒ¨í„´ 2: HTML ìš”ì†Œì—ì„œ ì¿ í° ì •ë³´ ì¶”ì¶œ
        for selector in coupon_selectors:
            coupon_elements = soup.select(selector)
            for elem in coupon_elements:
                if not elem:
                    continue
                    
                coupon = {
                    "discount_rate": 0,
                    "min_amount": 0,
                    "valid_until": None,
                    "description": ""
                }
                
                # ì¿ í° í…ìŠ¤íŠ¸ ì¶”ì¶œ - ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                discount_text = ""
                try:
                    discount_text = elem.get_text(strip=True)
                    if not discount_text:
                        continue
                except (AttributeError, TypeError):
                    continue
                
                # discount_textê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if not discount_text:
                    continue
                
                # í• ì¸ìœ¨ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´) - ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›
                discount_pattern = self._create_jp_kr_regex("å‰²å¼•", "í• ì¸")
                discount_patterns = [
                    r'(\d+)%off',
                    r'(\d+)%OFF',
                    f'(\\d+)%{discount_pattern}',
                    r'(\d+)%',
                    r'off\s*(\d+)%',
                    f'{discount_pattern}\\s*(\\d+)%'
                ]
                
                for pattern in discount_patterns:
                    discount_match = re.search(pattern, discount_text, re.I)
                    if discount_match:
                        coupon["discount_rate"] = int(discount_match.group(1))
                        break
                
                # ìµœì†Œ ê¸ˆì•¡ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´) - ì¼ë³¸ì–´-í•œêµ­ì–´ ëª¨ë‘ ì§€ì›
                above_pattern = self._create_jp_kr_regex("ä»¥ä¸Š", "ì´ìƒ")
                above_purchase_pattern = self._create_jp_kr_regex("ä»¥ä¸Šè³¼å…¥", "ì´ìƒêµ¬ë§¤")
                amount_patterns = [
                    f'(\\d{{1,3}}(?:,\\d{{3}})*)[,å††]{above_pattern}',
                    f'(\\d+)[,å††]{above_pattern}',
                    f'(\\d{{1,3}}(?:,\\d{{3}})*)[,å††]{above_pattern}ã®',
                    f'(\\d+)[,å††]{above_pattern}ã®',
                    f'(\\d{{1,3}}(?:,\\d{{3}})*)[,å††]{above_purchase_pattern}',
                    f'(\\d+)[,å††]{above_purchase_pattern}'
                ]
                
                for pattern in amount_patterns:
                    amount_match = re.search(pattern, discount_text)
                    if amount_match:
                        amount_str = amount_match.group(1).replace(',', '')
                        coupon["min_amount"] = int(amount_str)
                        break
                
                # ìœ íš¨ ê¸°ê°„ ì¶”ì¶œ
                date_patterns = [
                    r'(\d{4}\.\d{2}\.\d{2})\s*[~ã€œ]\s*(\d{4}\.\d{2}\.\d{2})',
                    r'(\d{4}-\d{2}-\d{2})\s*[~ã€œ]\s*(\d{4}-\d{2}-\d{2})',
                    r'(\d{4}/\d{2}/\d{2})\s*[~ã€œ]\s*(\d{4}/\d{2}/\d{2})'
                ]
                
                for pattern in date_patterns:
                    date_match = re.search(pattern, discount_text)
                    if date_match:
                        coupon["valid_until"] = date_match.group(2)
                        break
                
                # ì¿ í° ì„¤ëª… - ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                coupon["description"] = discount_text[:100] if discount_text else ""  # ìµœëŒ€ 100ì
                
                # ì¤‘ë³µ ì œê±° (í• ì¸ìœ¨ê³¼ ìµœì†Œ ê¸ˆì•¡ì´ ê°™ì€ ì¿ í°)
                coupon_key = f"{coupon['discount_rate']}_{coupon['min_amount']}"
                if coupon_key not in seen_coupons and coupon["discount_rate"] > 0:
                    coupons.append(coupon)
                    seen_coupons.add(coupon_key)
        
        return coupons
    
    def get_statistics(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ"""
        return self.db.get_crawling_statistics()